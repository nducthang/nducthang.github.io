<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>LLM on Thang&#39;s Blog</title>
    <link>https://nducthang.github.io/tags/llm/</link>
    <description>Recent content in LLM on Thang&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 10 Jun 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://nducthang.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[LLM 02] Data cleaning and Tokenizations</title>
      <link>https://nducthang.github.io/posts/2024-06-10-components-of-llm/</link>
      <pubDate>Mon, 10 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://nducthang.github.io/posts/2024-06-10-components-of-llm/</guid>
      <description>This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families
1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.
1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models.</description>
    </item>
    
    <item>
      <title>[LLM 01] Large Language Model Overview</title>
      <link>https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/</guid>
      <description>This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.
1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs.</description>
    </item>
    
  </channel>
</rss>
