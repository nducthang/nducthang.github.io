[{"content":"This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.\n1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs. Based on Transformer, we can split neural networks in LLM into 3 categories:\n Encoder-Only Decoder-Only Encoder-Decoder  1.1 Encoder-Only The models only consist of an encoder network. Representative encode-only models include BERT and its variants (RoBERTa, DeBERTa, XLM, ALBERT, …)\n BERT (Bidirectional Encoder Representations from Transformers) is one of the most widely used encode-only. The pre-trained BERT can be fine-tuned for different tasks like classification, question answering…  BERT consists of 3 modules:\n Embedding module: Convert input text into a sequence of embedding vectors. Stack of Transformer encoders: Converts embedding vectors into contextual representation vectors. Fully connected layer: Converts the representation vector (at the final layer) to one-hot vectors.  BERT training with 2 objectives.\n Masked Language Modeling (MLM) Next Sentence prediction (NSP)   RoBERTa: improves the robustness of BERT by modifying a few key hyperparameters, removing NSP, and enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. ALBERT uses 2 parameter-reduction technologies to lower memory consumption and increase the training speed of BERT:  Splitting the embedding matrix into 2 smaller matrices. Using repeating layers split among groups.   DeBERTa (Decoding enhanced BERT with disentangled attention): improve BERT and RoBERTa use 2 novel techniques:  The disentangled attention mechanism: Each word is represented using 2 vectors that encode its content and position. An enhanced mask recorder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. l adversarial training method is used for fine-tuning to improve models’ generalization.   ELECTRAL: New pre-trained task - Replaced token detection (RTD): Instead of masking the input, RDT corrupts it by replacing some tokens with plausible alternatives sampled from a small generator. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not. XLM: Extended BERT to cross-lingual language models using 2 methods:  An Unsupervised method that only relies on monolingual data. A supervised method that leverages parallel data with a new cross-lingual language model objective.    1.2 Decoder-Only The most widely used decoder-only are GPT-1 and GPT-2, these models lay the foundation for more powerful LLMs subsequently (i.e. GPT-3, GPT-4).\n GPT-1: (Generative Pre-Training) is a decoder-only Transformer model on diverse corpus of unlabeled text in self-supervised learning fashion, followed by discriminative fine-tuning on each specific downstream tasks.   GPT-2: shows that language models are able to learn to perform specific natural language tasks without any explicit supervision when trained on a large WebText dataset consisting of millions of webpages. The GPT-2 model follows the model designs of GPT-1 with a few modifications:  Layer normalization is moved to the input of each sub-block, additional layer normalization is added after the final self-attention block. Context size is increased from 512 to 1024 tokens.    1.3 Encoder-Decoder The most widely used encoder-decoder are T5, mT5, MASS and BART.\n T5: Text-to-Text Transfer Transformer (T5) model where transfer learning is effectively exploited for NLP via an introduction of a unified framework in which all NLP tasks are cast as a text-to-text generation task. mT5: pre-trained on a newcommon Crawl based dataset consisting of texts in 101 languages. MASS: MAsked Sequence to Sequence pre-training: adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence. The encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and the decoder predicts the masked fragment. BART: sequence-to-sequence translation model architecture, is a pre-trained by corrupting text with an arbitrary nosing function, and then learning to reconstruct the original text.  2. Large Language Model Families In this section, I review 3 LLM families: GPT, LLaMA, PaLM. In addition to these models, there are other popular LLMs which do not belong to those three model families, yet they have achieved great performance and have pushed the LLMs field forward: FLAN, Gopher, T0, ERNIE 3.0, RETRO, GlaM, LaMDA, OPTChinchilla, Galactica, CodeGen, AlexaTM, Sparrow, Minerva, MoD, BLOOM, GLM, Pythia, Orca, StarCoder, Kosmos, Gemini,…\n2.1 GPT Family This family consists of GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, GPT-4, CODEX, and WebGPT, developed by OpenAI.\n GPT-3: is a pre-trained autoregressive language model with 175 million parameters. GPT-3 shows the emergent ability of in-context learning, which means GPT-3 can be applied to any downstream tasks without any gradient updates or fine-tuning. CodeX: is a general-purpose programming model that can parse natural language and generate code in response. Fine-tuned for programming applications on code corpora collected from GitHub. WebGPT: fine-tuned to answer open-ended questions using a text-based web browser, facilitating users to search and navigate the web. WebGPT is trained in 3 steps:  Firstly, WebGPT learn to mimic human browsing behaviors using human demonstration data. Secondly, a reward funtion is leared to predict human preferences. Finally, WebGPT is refined to optimize the reward function via reinforcemence learning and rejction sampling.   InstructGPT: is proposed to align language models with user intent on a wide range of tasks by fune-tuning with human feedback. The method is called Reinforcement Learning from Human Feedback (RLHF).  GPT-4: is a multimodal LLM in that it can take image and text as inputs and produce text outputs. GPT-4 was first pre-trained to predict next tokens on large text corpora, and then fine-tuned with RLHF to align model behaviors with human-desired ones.  2.2 LLaMA Family LLaMA is a collection of foundation language models, released by Meta. Unlike GPT models, LLaMA models are open-source.\n LLaMA: uses the transformer architecture of GPT-3, with a few minor architectural modifications, including:  Using a SwiGLU activation function instead of ReLU. Using rotary postional embeddings instead of absolute positional embedding. Using root-mean-squared layer normalization instead of standard layer nomialization.   LLaMA 2: include both foundation language models and Chat models funetuned for dialog (LLaMA-2 Chat)  Alpaca: is fune-tuned from the LLaMA-7B model using 52K instruction-following demonstations generated in style of self-instruct using GPT-3.5. Vicuna: fine-tuning LLaMA on user-shared conversations. Guanaco: also finetuned LLaMA models using instruction-following data. But the funetuning a 65B done a a single 48GB GPU with QLoRA. QLoRA back-propagates gradients through a frozen, 4-bit quantized pre-trained language model into Low Rank Adapters (LoRA). Mistral-7B: is a 7B parameter language model. . This model leverages grouped-query attention for faster inference, coupled with sliding window attention to effectively handle sequences of arbitrary length with a reduced inference cost.  2.3 PaLM Family The PaLM (Pathways Language Model) family are developed by Google. It is a 540B parameter transformer-based LLM, trained on high-quality text corpus consistaning of 780 billion tokens.\n U-PaLM: models of 8B, 62B, and 540B scales arecontinually trained on PaLM with UL2R, a method of continue training LLMs on a few steps with UL2’s mixture-of-denoiser objective. PaLM-2: is trained using a mixture of objectives. Through extensive evaluations on English, multilingual, and reasoning tasks. Med-PaLM: is a medical questions domainm finetuned on PaLM using instruction prompt tuning, a parameter-effectient method for aligning LLMs to new domains using a few examples.    Shervin Minaee et al. “Large Language Models: A Survey”. In: arXiv preprint arXiv:2402.06196 (2024).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/","summary":"This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.\n1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs.","title":"Large Language Model Families"},{"content":"    I strongly encourage you to explore my earlier article, Understanding the Influence Function, as it serves as a valuable foundation for comprehending the content presented in this piece.\n What is error detection problem? The rapid growth of the internet, however, causes data to rise exponentially, posing numerous issues. Deep learning algorithms become less effective when big data is mislabeled or contains many errors. Current studies focus solely on improving the model rather than detecting data issues. Research on error tracing has not achieved high accuracy and is costly to store and compute, and is even void of stability or dependent on many different constraints and assumptions.\nThe model will work more accurately if the dataset is improved. Therefore, we concentrate on seeking solutions for resolving the problem of error tracing. We have a data set used to train the model, but it is enormous or requires a lot of expertise (such as in medical and engineering sectors); thus, people cannot manually check it. Besides, we will not be able to figure out how many incorrect patterns there are. Error detection problem to identify wrong data patterns in the original data set that we can modify to help improve the performance of deep learning models. The method in error dection problem must have high accuracy and fast access speed, minimize dependence on theoretical hypotheses, and be generalizable, suited for a wide range of data and deep learning models.\nIn this article, I will overview an approach to the error detection problem which is gradient-based methods, of which the Influence funtion in my previous article is one of those methods.\nRepresenter Points Representer Points 1 introduced by Yeh et al to approximate the influence function of training points on a test sample. To indicate the form of a representer theorem, suppose we solve for the optimal parameters: $$ \\theta^{*} = \\underset{\\theta \\in \\Theta}{\\text{argmin}} \\left\\{ \\frac{1}{n} \\sum_i^n L(x_i,y_i, \\theta) + g(\\| \\theta \\|) \\right\\} $$ where \\(g\\) is non-decreasing, \\(L_2\\) regularized. The parameterization spliting of the model as a feature model and a prediction network:  Feature model: \\(\\phi_2(x_i, \\theta_2)\\) can be arbitrarily deep, or simply the identify function. Prediction network: with parameters \\(\\theta_1\\) is \\(\\phi(x_i, \\theta) = \\theta_1 f_i \\subseteq \\mathbb{R}^c\\) and \\(f_i = \\phi_2 (x_i, \\theta_2) \\subseteq \\mathbb{R}^f\\) is the last intermedicate layer feature in the neural network for input \\(x_i\\). So, we have a prediction of model is \\(\\hat{y}_i = \\sigma\\left( \\phi(x_i, \\theta) \\right)\\) and let \\(g(\\| \\theta \\|) = \\lambda \\| \\theta_1 \\|^2\\). Then, we have the decomposition: $$ \\phi(x_t, \\theta^{*}) = \\sum_{i}^n k(x_t, x_i, \\alpha_i) $$ where \\(\\alpha_{i}=\\frac{1}{-2 \\lambda{n}} \\frac{\\partial \\mathcal{L}\\left(x_{i}, y_{i}, \\theta\\right)}{\\partial \\phi\\left(x_{i}, \\theta\\right)}\\) and \\(k(x_t, x_i, \\alpha_i)=\\alpha_i f_i^T f_t\\), which we call a representer value for \\(x_i\\) given \\(x_t\\). We showed that for such models the output for any target instance \\(x_t\\) can be expressed as a linear decomposition of “data importance” of training instances. Through this, we can evaluate when a testing point of the model predicts incorrectly which training points affect it the most, and expect that those training points are mislabeled data.   Proof: Note that for any stationary point, the gradient of the loss with respect to \\(\\theta_1\\) is equal to \\(0\\). We therefore have: $$ \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial L\\left({x}_{i}, {y}_{i}, {\\theta}\\right)}{\\partial {\\theta}_{1}}+2 \\lambda {\\theta}_{1}^{*}=0 \\quad \\Rightarrow \\quad {\\theta}_{1}^{*}=-\\frac{1}{2 \\lambda n} \\sum_{i=1}^{n} \\frac{\\partial L\\left({x}_{i}, {y}_{i}, {\\theta}\\right)}{\\partial {\\theta}_{1}}=\\sum_{i=1}^{n} \\alpha_{i} {f}_{i}^{T} $$ where \\(\\alpha_{i}=\\frac{1}{-2 \\lambda{n}} \\frac{\\partial \\mathcal{L}\\left(x_{i}, y_{i}, \\theta\\right)}{\\partial \\phi\\left(x_{i}, \\theta\\right)}\\) by the chain rule. We thus have that: $$ \\phi(x_t, \\theta^{*}) = \\theta_1^{*} f_t = \\sum_{i}^n k(x_t, x_i, \\alpha) $$ where \\(k(x_t, x_i, \\alpha_i) = \\alpha_i f_i^T f_t\\) by simply plugging in the two above expression. Gradient Dot and Gradient Cosine Similarity These methods are introduced by Charpiat et al 2. Basically, these methods are based on influence function. If one wants to change the value of \\(L(z, \\theta)\\) by a small quantity \\(\\epsilon\\), one needs to update \\(\\theta\\) by \\(\\delta \\theta = \\epsilon \\frac{\\nabla_{\\theta}L(z, \\theta)}{\\| \\nabla_{\\theta} L(z, \\theta) \\|^2}\\). Indeed, after the parameter update, the new value at \\(z\\) will be: $$ L(z, \\theta + \\delta \\theta)(z) = L(z, \\theta) + \\nabla L(z, \\theta) \\cdot \\delta \\theta + O( \\| \\delta \\theta \\|) \\\\ = L(z, \\theta) + \\epsilon + O(\\epsilon^2) $$ This parameter change induces a value change at any other point \\(z'\\): $$ L(z', \\theta + \\delta \\theta) = L(z', \\theta) + \\nabla L(z', \\theta) \\cdot \\delta \\theta +O(\\| \\delta \\theta \\|) \\\\ = L(z', \\theta) + \\epsilon \\frac{\\nabla L(z', \\theta) \\cdot \\nabla L(z, \\theta)}{\\| \\nabla L(z, \\theta) \\|^2 } + O(\\epsilon^2) $$ Therefore the kernel represents the influence of $z$ over $z'$: $$ k_{\\theta}^N (z, z') = \\frac{\\nabla L(z', \\theta) \\cdot \\nabla L(z, \\theta)}{\\| \\nabla L(z, \\theta) \\|^2 } $$ Note however that \\(k_{\\theta}^N (x, x')\\) is not symmetric. We have two symmetric kernels natural arise:  The inner product: (We called it Gradient Dot) $$ k_{\\theta}^I (z, z') = \\nabla_{\\theta} L(z, \\theta) \\cdot \\nabla L(z', \\theta) $$  Normalized version: (We called it Gradient Cosine Similarity) $$ k_{\\theta}^C (z, z') = \\frac{\\nabla L(z, \\theta)}{\\|\\nabla L(z, \\theta) \\| } \\cdot \\frac{\\nabla L(z',\\theta)}{\\| \\nabla L(z', \\theta) \\|} $$ It can be seen that \\(k_{\\theta}^C (z, z')\\) has the advantage of being bounded (in \\([-1, 1]\\), thus expressing similarity in a usual meaning. Interestingly, \\(k_{\\theta}^I\\) is equivalent to the Influence function without Hessian, in An Empirical Comparison of Instance Attribution Methods for NLP 3 experimented and concluded that this formula is roughly equivalent to influence function with Hessian.  RelatIF method One shortcoming of influence functions is that the training examples deemed most influential are ofter outliers or mislabeled, making them poor choices for an explanation. RelatIF 4 is a new class of criteria for choosing relevant training examples by way of an optimization objective that places a constraint on global influence. Above figure described binary classification by linear decision boundaries (dashed line) to illustrate the difference between IF and RelatIF.\nIn figure (a), we can see that the model predicts the test input (star). As estimated by IF, the most influential training example for the prediction is an outlier (circled). Using RelatIF, the most effective training example is more typical (encased in a square).\nFigure (b) see that using IF, every test input falling within the shaded yellow region is most influenced by the same outlier (circled). Test inputs in the remaining white area are most influenced by one of 5 other high loss examples.\nFigure (c) using RelatIF, the area where test inputs are most affected by the outlier (circled) shrinks. Test inputs in the remaining region are most influenced by one of 65 other examples. To evaluate the effect of a training example \\(z_i\\) on testing example \\(z_{\\text{test}}\\), we have: $$ \\textbf{RelatIF}(z_i, z_{\\text{test}}) = \\cos \\left(H^{-\\frac{1}{2}} \\nabla_{\\theta} L(z_{\\text{test}}), H^{-\\frac{1}{2}} \\nabla_{\\theta} L(z_i) \\right) $$ where \\(\\cos\\) is cosine similarity function. TracIn method Introduction TracIn method 5 introduced by Garima and Freferick Liu et al. It computes the influence of a training example on a prediction made by the model. It applies to any machine learning model trained using stochastic gradient descent or variant, agnostic architecture, domain, and task.\nThe idealized notion of the influence of a particular training example \\(z\\) in a given test example \\(z'\\) is defined as the total reduction in loss on the test example \\(z'\\) that is induced by the training process whenever the training example \\(z\\) is utilized: $$ \\text{TracInIdeal(z,z')} = \\sum_{t:z_t =z} L(z', \\theta_t) - L(z', \\theta_{t+1}) $$ where \\(\\theta_t\\) and \\(\\theta_{t+1}\\) is parameters of model when using training example \\(z_t\\) with SGD to updating from \\(\\theta_t\\) to \\(\\theta_{t+1}\\). Suppose the initial parameter vector before starting the training process is \\(\\theta_0\\), and the final parameter vector is \\(\\theta_T\\). The iterative optimization technique operates on one training example at a time. Then: $$ \\sum_{i=1}^n \\text{TracInIdeal}(z_i,z') = L(z', \\theta_0) - L(z', \\theta_T) $$ Let:\n Proponents: Training samples that have positive value to influence score (reducing loss). Or so-called helpful/excitatory samples. Opponents: Training samples that have negative value to influence score (increase loss). Or so-called harmful/inhibitor samples   First-order Approximation to Idealized Influence We can approximate the change in the loss: $$ L(z',\\theta_{t+1}) = L(z',\\theta_t) + \\nabla L(z', \\theta_t) \\cdot (\\theta_{t+1} - \\theta_t) + O(\\|\\theta_{t+1} - \\theta_t \\|^2) $$ Using the training point \\(z_t\\) at iteration \\(t\\), then the change in parameters: $$ \\theta_{t+1} - \\theta_t = -\\eta_t \\nabla L(z_t, \\theta_t) $$ where \\(\\eta_t\\) is the step size in iteration \\(t\\). This formula should be changed appropriately if other optimization methods as AdaGrad, Adam, Newton,\\(\\ldots\\\\\\) Combining 2 above equations, we have: $$ L(z', \\theta_{t}) - L(z', \\theta_{t+1}) \\approx \\eta_t \\nabla L(z',\\theta_t) \\cdot (\\theta_{t+1} - \\theta_t) $$ The effect can be approximated for a training sample $z$ by summing the approximations over all iterations, where \\(z\\) is used to update the parameter. $$ \\text{TracIn}(z, z') = \\sum_{t:z_t=z} \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t) $$ Extension to Minibatches To handle minibatches of size \\(b \\ge 1\\), we compute the influence of a minibatch on the test point \\(z'\\), and then take its first-order approximation: First-Order Approximation \\((B_t, z') = \\frac{1}{b} \\sum_{z \\in B_t} \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t) \\), because the gradient for the minibatch \\(B_t\\) is \\(\\frac{1}{b} \\sum_{z \\in B_t} \\nabla L(z, \\theta_t)\\). Then, for each training point \\(z \\in B_t\\), we attribute the \\(\\frac{1}{b} \\cdot \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t)\\) portion of the influence of \\(B_t\\) on the test point \\(z'\\). Summing up over all iteratios \\(t\\) in which a particular training point \\(z\\) was chosen in \\(B_t\\), we arrive at the following definition of TracIn with minibatches: $$ \\text{TracIn}(z, z') = \\frac{1}{b} \\sum_{t:z \\in B_t} \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t) $$ TracIn with Checkpoint Suppose we have \\(k\\) checkpoints \\(\\theta_{t_1}, \\theta_{t_2},\\ldots,\\theta_{t_k}\\). We assume that between checkpoints, each training example is visited exactly once. And we use the notation \\(\\eta_i\\) to denote the step size between checkpoints \\(i-1\\) and \\(i\\). While the first-order approximation of the influence needs the parameter vector at the specific iteration where a given training example is visited, since we don't have access to the parameter vector, we approximate it with the first checkpoint parameter vector after it. Thus, this heuristic results: $$ \\text{TracInCP}(z,z') = \\sum_{i=1}^k \\eta_i \\nabla L(z, \\theta_{t_i}) \\cdot \\nabla L(z', \\theta_{t_i}) $$ Summary Through this article, I have introduced you to gradient-based methods employed in error detection tasks. While fundamentally providing satisfactory results, these gradient-based approaches demand significant computational resources and storage capacity.\n  Chih-Kuan Yeh et al. “Representer point selection for explaining deep neural networks”. In: arXiv preprint arXiv:1811.09720 (2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Guillaume Charpiat et al. “Input similarity from the neural network perspective”. In: NeurIPS 2019-33th Annual Conference on Neural Information Processing Systems. 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Pouya Pezeshkpour et al. “An Empirical Comparison of Instance Attribution Methods for NLP”. In: arXiv preprint arXiv:2104.04128 (2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Elnaz Barshan, Marc-Etienne Brunet, and Gintare Karolina Dziugaite. “Relatif:Identifying explanatory training samples via relative influence”. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 1899–1909.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Garima Pruthi et al. “Estimating training data influence by tracing gradient descent”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 19920–19930.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nducthang.github.io/posts/2023-11-25-gradient-based-methods/","summary":"I strongly encourage you to explore my earlier article, Understanding the Influence Function, as it serves as a valuable foundation for comprehending the content presented in this piece.\n What is error detection problem? The rapid growth of the internet, however, causes data to rise exponentially, posing numerous issues. Deep learning algorithms become less effective when big data is mislabeled or contains many errors. Current studies focus solely on improving the model rather than detecting data issues.","title":"Gradient-based methods in error detection"},{"content":"   In this article, I review about Influence functions and various of its - a classic technique from robust statistics - to trace a model\u0026rsquo;s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction.\nBasics of influence function Consider a prediction problem from some input space \\(\\mathcal{X}\\) (e.g., images, text,\\(\\ldots\\)) to an output space \\(\\mathcal{Y}\\) (e.g,. labels). We are given training points \\(z_1,\\ldots,z_n\\), where \\(z_i=(x_i,y_i) \\in \\mathcal{X} \\times \\mathcal{Y}\\). For a point \\(z\\) and parameters \\(\\theta \\in \\Theta\\), let \\(L(z, \\theta)\\) be the loss, and let \\(\\frac{1}{n} \\sum_{i=1}^n L(z_i, \\theta)\\) be the empirical risk. The empirical risk minimizer is given by $$ \\hat{\\theta} \\stackrel{\\text { def }}{=} \\arg \\min _{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L\\left(z_{i}, \\theta\\right) $$ Assume that the empirical risk is twice-differentiable and strictly convex in \\(\\theta\\). Our goal is to understand the effect of training points on a model's predictions. Up-weighting a training point  Up-weighting a training example \\(z\\) by an infinitesimal amount \\(\\epsilon\\) leads to a new set of model parameters denoted by \\(\\hat{\\theta}_{\\epsilon, z}\\). This set of new model parameters \\(\\hat{\\theta}_{\\epsilon, z}\\) is obtained by solving: $$ \\hat{\\theta}_{\\epsilon, z} \\stackrel{\\text { def }}{=} \\arg \\min _{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L\\left(z_{i}, \\theta\\right)+\\epsilon L(z, \\theta) $$ Removing a training point \\(z\\) is similar to up-weighting its corresponding weight by \\(\\epsilon = \\frac{-1}{n}\\) in below equation. The main idea used is to approximate \\(\\hat{\\theta}_{\\epsilon, z}\\) by the first-order Taylor series expansion around the optimal model parameters represented by \\(\\theta^{*}\\), which leads to: $$ \\hat{\\theta}_{\\epsilon, z} \\approx \\theta^{*} - \\epsilon H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta}) $$ where \\(H_{\\hat{\\theta}} \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\theta}^{2} L\\left(z_{i}, \\hat{\\theta}\\right)\\) is the Hessian and is positive definite (PD) by assumption. Following the result in Pang Wei Koh and Percy Liang1, the change in the model parameters (\\(\\Delta\\theta = \\hat{\\theta}_{-z} - \\hat{\\theta} \\approx - \\frac{1}{n} \\mathcal{I}_{\\text {up,params }} (z)\\): $$ \\left.\\mathcal{I}_{\\text {up,params }}(z) \\stackrel{\\text { def }}{=} \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0}=-H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta}) $$ The change in the loss value for a particular test point \\(z_t\\) when a training point \\(z\\) is up-weighted can be approximated as a closed-form expression by the chain rule: $$ \\begin{aligned} \\mathcal{I}_{\\text {up,loss }}\\left(z, z_{\\text {test }}\\right) \u0026\\left.\\stackrel{\\text { def }}{=} \\frac{d L\\left(z_{\\text {test }}, \\hat{\\theta}_{\\epsilon, z}\\right)}{d \\epsilon}\\right|_{\\epsilon=0} \\\\ \u0026=\\left.\\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0} \\\\ \u0026=-\\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta}) . \\end{aligned} $$ Below equation is approximately the change in the loss for the test-sample \\(z_{\\text{test}}\\) when a training sample \\(z\\) is removed from the training set. In addition, in the paper 1, the authors also introduce Perturbing a training input. However, it is less common than up-weighting a training point.\nDeriving the influence function \\(\\mathcal{I}_{\\text{up, params}}\\) Recall that \\(\\hat{\\theta}\\) minimizes the empirical risk: $$ R(\\theta) \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^{n} L\\left(z_{i}, \\theta\\right) $$ We further assume that \\(R\\) is twice-differentable and strongly convex in \\(\\theta\\), i.e: $$ H_{\\hat{\\theta}} \\stackrel{\\text { def }}{=} \\nabla^{2} R(\\hat{\\theta})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\theta}^{2} L\\left(z_{i}, \\hat{\\theta}\\right) $$ exits and is positive definite. This guarantees the existence of \\(H_{\\hat{\\theta}}^{-1}\\), which we will use in the subsequent derivation. The perturbed parameters \\(\\hat{\\theta}_{\\epsilon, z}\\) can be written as: $$ \\hat{\\theta}_{\\epsilon, z}=\\arg \\min _{\\theta \\in \\Theta}\\{R(\\theta)+\\epsilon L(z, \\theta)\\} $$ Define the parameter change \\(\\Delta_{\\epsilon} = \\hat{\\theta}_{\\epsilon,z} - \\hat{\\theta}\\), and note that, as \\(\\hat{\\theta}\\) doesn't depend on \\(\\epsilon\\), the quantity we seek to compute can be written in terms of it: $$ \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}=\\frac{d \\Delta_{\\epsilon}}{d \\epsilon} $$ Since \\(\\hat{\\theta}_{\\epsilon, z}\\) is a minimizer of above equation, let us examine its first-order optimality conditions: $$ 0=\\nabla R\\left(\\hat{\\theta}_{\\epsilon, z}\\right)+\\epsilon \\nabla L\\left(z, \\hat{\\theta}_{\\epsilon, z}\\right) $$ Next, since \\(\\hat{\\theta}_{\\epsilon, z} \\to \\hat{\\theta}\\) as \\(\\epsilon \\to 0\\), we perform a Taylor expansion of the right-band side: $$ \\begin{aligned} 0 \\approx \u0026[\\nabla R(\\hat{\\theta})+\\epsilon \\nabla L(z, \\hat{\\theta})]+\\\\ \u0026\\left[\\nabla^{2} R(\\hat{\\theta})+\\epsilon \\nabla^{2} L(z, \\hat{\\theta})\\right] \\Delta_{\\epsilon} \\end{aligned} $$ where we have dropped \\(o\\left(\\left\\|\\Delta_{\\epsilon}\\right\\|\\right)\\) terms. Solving for \\(\\Delta_{\\epsilon}\\), we get: $$ \\begin{array}{c} \\Delta_{\\epsilon} \\approx-\\left[\\nabla^{2} R(\\hat{\\theta})+\\epsilon \\nabla^{2} L(z, \\hat{\\theta})\\right]^{-1} \\\\ {[\\nabla R(\\hat{\\theta})+\\epsilon \\nabla L(z, \\hat{\\theta})]} \\end{array} $$ since \\(\\hat{\\theta}\\) minimizes \\(R\\), we have \\(\\nabla R(\\hat{\\theta})=0\\). Dropping \\(o(\\epsilon)\\) terms, we have: $$ \\Delta_{\\epsilon} \\approx-\\nabla^{2} R(\\hat{\\theta})^{-1} \\nabla L(z, \\hat{\\theta}) \\epsilon $$ So, we conclude that: $$ \\begin{aligned} \\left.\\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0} \u0026=-H_{\\hat{\\theta}}^{-1} \\nabla L(z, \\hat{\\theta}) \\\\ \u0026 \\stackrel{\\text { def }}{=} \\mathcal{I}_{\\text {up,params }}(z) \\end{aligned} $$ Efficiently calculating influence Hessian-vector products  It is not easy and expensive to compute the matrix \\(H_{\\theta}^{-1}\\) directly. We can use Hessian-vector products (HVPs) to efficiently approximate \\(s_{\\text {test }} \\stackrel{\\text { def }}{=} H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)\\) and then compute \\(\\mathcal{I}_{\\text {up,loss }}\\left(z, z_{\\text {test }}\\right)=-s_{\\text {test }} \\cdot \\nabla_{\\theta} L(z, \\hat{\\theta})\\). With \\(s_{\\text{test}}\\), we can recursively compute the following: $$ \\left\\{ \\begin{matrix} H^{-1}_{0} v = v \\\\ H_j^{-1}v = v + (I-\\nabla^2_{\\theta} L(z_{s_j}, \\hat{\\theta})) H_{j-1}^{-1}v \\end{matrix} \\right. $$ Initialization: \\(v = \\nabla_{\\theta} L(z_{test}, \\hat{\\theta})\\), the \\(z_{s_j}\\) points are randomly taken from the training set. Equation equivalent: $$ \\left\\{ \\begin{matrix} H^{-1}_{0} v = v \\\\ H_j^{-1}v = v + IH_{j-1}^{-1}v - \\nabla^2_{\\theta} L(z_{s_j}, \\hat{\\theta}) H_{j-1}^{-1}v \\end{matrix} \\right. $$ The third component of the above equation is equivalent to calculating \\(\\mathbf{Hv}\\) where: \\(\\mathbf{H}=\\nabla^2_{\\theta} L(z_{s_j}, \\hat{\\theta})\\) and \\(\\mathbf{v}=H_{j-1}^{-1}v\\). In Heterogeneous uncertainty sampling for supervised learning2, we have: $$ \\mathbf{H} \\mathbf{v}=\\nabla_{\\mathbf{\\theta}}\\left(\\mathbf{v} \\cdot \\nabla_{\\mathbf{\\theta}} L\\right) $$ In influence function calculated as above, cost and approximation quality depends:\n J: the number of recursive iterations. T: the number of independent runs. B: the batch size of sample points from training data (Number of \\(z_{s_j}\\) ). To improve the speed of influence function, the paper Fastif: Scalable influence functions for efficient model interpretation and debugging 3 suggest some ideas such as: Speeding up the argmax using kNN, Speeding up the Inverse Hessian, Parallelization.  Speeding up the argmax using kNN  With the influence function, we need to calculate influence on the full training data. $$ z^{*}=\\underset{z \\in \\mathcal{Z}}{\\arg \\max } \\mathcal{I}\\left(z, z_{\\text {test }}\\right) $$ We hypothesize that we could constrain the expensive search to a subset of promising data points, \\(\\hat{\\mathcal{Z}} \\subseteq \\mathcal{Z}\\): $$ z^{*}=\\underset{z \\in \\hat{\\mathcal{Z}}}{\\arg \\max } \\mathcal{I}\\left(z, z_{\\text {test }}\\right) $$ We can select subset \\(\\hat{\\mathcal{Z}}\\) as the top-k nearest neighbors of \\(z_{\\text{test}}\\) based on the \\(L_{2}\\) distance between extracted features of the data-points and can use libraries such as FAISS 4.\nTo evaluate, define the recall score R@m as the percentage of top-m ground-truth influential data points selected by the kNN. $$ R @ m=\\frac{\\mid\\{\\text { retrieved }\\} \\cap\\{\\text { top- } m \\text { influential }\\} \\mid}{\\mid\\{\\text { top- } m \\text { influential } \\mid} $$ Speeding up the Inverse Hessian Propose a few simple changes:\n Choose a J so that approximation converges. Choose a small batch size. In our experiments, we found that even B = 1 suffices. Make up for the noisiness of small batch size using larger T , which can be distributed over multiple GPUs.  Parallelization We can apply asynchronous parallel computation to compute the \\(s_{\\text{test}}\\), use one synchronization point to average the result using all-reduce, and then asynchronously compute the influence of a subset of data-points that are pre-selected using kNN Influence functions in deep learning are fragile The influence function method is unstable in deep learning networks. According to the theory of influence functions, they work well with convex functions. However, it is not clear for non-convex functions. In the paper Influence functions in deep learning are fragile 5 provides comparative experiments on influence functions in networks with different depths and with different data. Experimentally, the authors concluded that:\n Influence function estimates fairly accurately when networks are shallow, not so well in deeper networks. Train model with weight decay \\(\\lambda\\| \\theta \\|_2^2\\) into the loss function to increase accuracy. The accuracy of influence can vary considerably depending on the test points.  Based on theory and experiment, I see that the influence function does not work effectively with non-convex optimization problems. This is also the biggest drawback of this method.\n  Pang Wei Koh and Percy Liang. “Understanding black-box predictions via influence functions”. In: International Conference on Machine Learning.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n David D Lewis and Jason Catlett. “Heterogeneous uncertainty sampling for supervised learning”. In: Machine learning proceedings 1994. Elsevier, 1994, pp. 148–156.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Han Guo et al. “Fastif: Scalable influence functions for efficient model interpretation and debugging”. In: arXiv preprint arXiv:2012.15781 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Jeff Johnson, Matthijs Douze, and Herve Jegou. “Billion-scale similarity searchwith gpus”. In: IEEE Transactions on Big Data (2019).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Samyadeep Basu, Philip Pope, and Soheil Feizi. “Influence functions in deep learning are fragile”. In: arXiv preprint arXiv:2006.14651 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nducthang.github.io/posts/2023-11-24-influence-function/","summary":"In this article, I review about Influence functions and various of its - a classic technique from robust statistics - to trace a model\u0026rsquo;s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction.\nBasics of influence function Consider a prediction problem from some input space \\(\\mathcal{X}\\) (e.g., images, text,\\(\\ldots\\)) to an output space \\(\\mathcal{Y}\\) (e.g,. labels).","title":"What is Influence Function?"}]