[{"content":"This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families\n1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.\n1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models. Common data filtering techniques include:\n  Removing Noise: This involves eliminating irrelevant or noisy data that might impair the model\u0026rsquo;s ability to generalize. For instance, removing false information from the training data can reduce the likelihood of the model generating incorrect responses. Two mainstream approaches for quality filtering are classifier-based and heuristic-based frameworks.\n  Handling Outliers: Identifying and managing outliers or anomalies in the data to prevent them from disproportionately influencing the model. This ensures that the model learns from typical data patterns rather than skewed anomalies.\n  Addressing Imbalances: Balancing the distribution of classes or categories in the dataset to avoid biases and ensure fair representation. This is particularly important for responsible model training and evaluation.\n  Text Preprocessing: Cleaning and standardizing text data by removing stop words, punctuation, or other elements that may not contribute significantly to the model’s learning. This step ensures that the data is uniform and free of unnecessary noise.\n  Dealing with Ambiguities: Resolving or excluding ambiguous or contradictory data that might confuse the model during training. By clarifying these ambiguities, the model can provide more definite and reliable answers.\n  1.2 Deduplication Deduplication refers to the process of removing duplicate instances or repeated occurrences of the same data in a dataset. Duplicate data points can introduce biases in the model training process and reduce data diversity, as the model may overfit on those particular instances.\n1.3 Summary In summary, data cleaning, which includes both filtering and deduplication, is essential for improving the performance of language models. By ensuring high-quality, diverse, and representative training data, we can train more effective and reliable models. As demonstrated by the Falcon40B example, rigorous data cleaning can lead to significant advancements in model performance, highlighting the importance of these techniques in the development of large language models.\n2. Tokenizations Tokenization refers to the process of converting a sequence of text into smaller parts known as tokens. While the simplest tokenization tool merely splits text based on white space, more advanced tokenization tools rely on a word dictionary. However, this approach faces the challenge of out-of-vocabulary (OOV) words, as the tokenizer can only recognize words within its dictionary. To mitigate this issue and improve dictionary coverage, popular tokenizers for large language models (LLMs) are based on sub-words. These sub-words can be combined to form a large number of words, including those not seen during training or those in different languages. Here, we will explore three widely used tokenization methods: Byte Pair Encoding, WordPiece Encoding, and SentencePiece Encoding.\n2.1 Byte Pair Encoding (BPE) Byte Pair Encoding is a algorithm that uses frequent patterns at byte level to compress data. For example, Our corpus contain words:\nlow low low lower So, we have base vocabulary is:\n [\u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;] After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one. At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by “pair,” here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step. Iterations:\nVocabulary: [\u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;] Corpus: [(\u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;w\u0026#39;, 3), (\u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;, 1)] Then we look at pairs. The pair (\u0026rsquo;l\u0026rsquo;, \u0026lsquo;o\u0026rsquo;) is present in the word low and lower with total 4 times. So, we have:\nVocabulary: [\u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;lo\u0026#39;] Corpus: [(\u0026#39;lo\u0026#39;, \u0026#39;w\u0026#39;, 3), (\u0026#39;lo\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;, 1)] Then:\nVocabulary: [\u0026#39;l\u0026#39;, \u0026#39;o\u0026#39;, \u0026#39;w\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;, \u0026#39;lo\u0026#39;, \u0026#39;low\u0026#39;] Corpus: [(\u0026#39;low\u0026#39;, 3), (\u0026#39;low\u0026#39;, \u0026#39;e\u0026#39;, \u0026#39;r\u0026#39;, 1)] And we continue like this until we reach the desired vocabulary size.\nImplement: Detail\nAdvantages: BPE is effective in representing morphological variations of frequent words, as common prefixes and suffixes are well captured if they frequently appear in the training data.\nApplication: BPE is widely used in models where efficient vocabulary management and flexibility to handle new words are crucial.\n2.2 Word Piece Encoding WordPiece Encoding is primarily used in well-known models like BERT and Electra. It aims to address the issue of unknown tokens (UNK) by ensuring all characters in the training data are represented in the vocabulary.\nWordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, \u0026ldquo;word\u0026rdquo; gets split like this:\nw ##o ##r ##d Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula: $$score=(freq_of_pair)/(freq_of_first_element \\times freq_of_second_element)$$\nBy dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.\nExample: We have vocabulary and frequent of each words as follow:\n(\u0026#34;hug\u0026#34;, 10), (\u0026#34;pug\u0026#34;, 5), (\u0026#34;pun\u0026#34;, 12), (\u0026#34;bun\u0026#34;, 4), (\u0026#34;hugs\u0026#34;, 5) The most frequent pair is (\u0026quot;##u\u0026quot;, \u0026ldquo;##g\u0026rdquo;) (present 20 times), but the individual frequency of \u0026ldquo;##u\u0026rdquo; is very high, so its score is not the highest (it’s 1 / 36). All pairs with a \u0026ldquo;##u\u0026rdquo; actually have that same score (1 / 36), so the best score goes to the pair (\u0026quot;##g\u0026quot;, \u0026ldquo;##s\u0026rdquo;) — the only one without a \u0026ldquo;##u\u0026rdquo; — at 1 / 20, and the first merge learned is (\u0026quot;##g\u0026quot;, \u0026ldquo;##s\u0026rdquo;) -\u0026gt; (\u0026quot;##gs\u0026quot;).\nVocabulary: [\u0026#34;b\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;##g\u0026#34;, \u0026#34;##n\u0026#34;, \u0026#34;##s\u0026#34;, \u0026#34;##u\u0026#34;, \u0026#34;##gs\u0026#34;] Corpus: (\u0026#34;h\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##g\u0026#34;, 10), (\u0026#34;p\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##g\u0026#34;, 5), (\u0026#34;p\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##n\u0026#34;, 12), (\u0026#34;b\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##n\u0026#34;, 4), (\u0026#34;h\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##gs\u0026#34;, 5) Iterative: (\u0026ldquo;h\u0026rdquo;, \u0026ldquo;##u\u0026rdquo;) -\u0026gt; \u0026ldquo;hu\u0026rdquo;.\nVocabulary: [\u0026#34;b\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;##g\u0026#34;, \u0026#34;##n\u0026#34;, \u0026#34;##s\u0026#34;, \u0026#34;##u\u0026#34;, \u0026#34;##gs\u0026#34;, \u0026#34;hu\u0026#34;] Corpus: (\u0026#34;hu\u0026#34; \u0026#34;##g\u0026#34;, 10), (\u0026#34;p\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##g\u0026#34;, 5), (\u0026#34;p\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##n\u0026#34;, 12), (\u0026#34;b\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##n\u0026#34;, 4), (\u0026#34;hu\u0026#34; \u0026#34;##gs\u0026#34;, 5) Then the next best score is shared by (\u0026ldquo;hu\u0026rdquo;, \u0026ldquo;##g\u0026rdquo;) and (\u0026ldquo;hu\u0026rdquo;, \u0026ldquo;##gs\u0026rdquo;) (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:\nVocabulary: [\u0026#34;b\u0026#34;, \u0026#34;h\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;##g\u0026#34;, \u0026#34;##n\u0026#34;, \u0026#34;##s\u0026#34;, \u0026#34;##u\u0026#34;, \u0026#34;##gs\u0026#34;, \u0026#34;hu\u0026#34;, \u0026#34;hug\u0026#34;] Corpus: (\u0026#34;hug\u0026#34;, 10), (\u0026#34;p\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##g\u0026#34;, 5), (\u0026#34;p\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##n\u0026#34;, 12), (\u0026#34;b\u0026#34; \u0026#34;##u\u0026#34; \u0026#34;##n\u0026#34;, 4), (\u0026#34;hu\u0026#34; \u0026#34;##gs\u0026#34;, 5) and we continue like this until we reach the desired vocabulary size.\nImplement: Detail\nAdvantages: This method reduces the occurrence of unknown tokens, enhancing the model’s ability to handle diverse and previously unseen inputs.\nApplication: WordPiece Encoding is essential for models that require high accuracy in token representation and robust handling of diverse inputs.\n2.3 Unigram Encoding The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.\nUnigram Encoding is a probabilistic model that treats each token as an independent unit, selecting tokens based on their probability of occurrence in the training data. Unlike other methods that build tokens by iteratively merging characters or sub-words, Unigram Encoding starts with a large vocabulary and iteratively prunes it to find the optimal set of tokens.\nExample:\n(\u0026#34;hug\u0026#34;, 10), (\u0026#34;pug\u0026#34;, 5), (\u0026#34;pun\u0026#34;, 12), (\u0026#34;bun\u0026#34;, 4), (\u0026#34;hugs\u0026#34;, 5) Initial Vocabulary: Unigram Encoding begins with a large vocabulary that includes all possible tokens derived from the training data. This vocabulary often consists of individual characters, sub-words, and entire words.\n[\u0026#34;h\u0026#34;, \u0026#34;u\u0026#34;, \u0026#34;g\u0026#34;, \u0026#34;hu\u0026#34;, \u0026#34;ug\u0026#34;, \u0026#34;p\u0026#34;, \u0026#34;pu\u0026#34;, \u0026#34;n\u0026#34;, \u0026#34;un\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;bu\u0026#34;, \u0026#34;s\u0026#34;, \u0026#34;hug\u0026#34;, \u0026#34;gs\u0026#34;, \u0026#34;ugs\u0026#34;] Probability Assignment: Each token in the initial vocabulary is assigned a probability based on its frequency of occurrence in the training data. These probabilities help in evaluating the importance of each token.\nFor Example, Here are the frequencies of all the possible subwords in the vocabulary:\n(\u0026#34;h\u0026#34;, 15) (\u0026#34;u\u0026#34;, 36) (\u0026#34;g\u0026#34;, 20) (\u0026#34;hu\u0026#34;, 15) (\u0026#34;ug\u0026#34;, 20) (\u0026#34;p\u0026#34;, 17) (\u0026#34;pu\u0026#34;, 17) (\u0026#34;n\u0026#34;, 16) (\u0026#34;un\u0026#34;, 16) (\u0026#34;b\u0026#34;, 4) (\u0026#34;bu\u0026#34;, 4) (\u0026#34;s\u0026#34;, 5) (\u0026#34;hug\u0026#34;, 15) (\u0026#34;gs\u0026#34;, 5) (\u0026#34;ugs\u0026#34;, 5) So, the sum of all frequencies is 210, and the probability of the subword \u0026ldquo;ug\u0026rdquo; is thus 20/210.\nIterative Pruning: The algorithm iteratively removes tokens that contribute the least to the model\u0026rsquo;s likelihood, recalculating probabilities at each step. This process continues until an optimal vocabulary size is achieved, balancing model complexity and performance.\nFor example, \u0026ldquo;pug\u0026rdquo; has the probability:\n[\u0026#34;p\u0026#34;, \u0026#34;u\u0026#34;, \u0026#34;g\u0026#34;] : 0.000389 [\u0026#34;p\u0026#34;, \u0026#34;ug\u0026#34;] : 0.0022676 [\u0026#34;pu\u0026#34;, \u0026#34;g\u0026#34;] : 0.0022676 So, \u0026ldquo;pug\u0026rdquo; would be tokenized as [\u0026ldquo;p\u0026rdquo;, \u0026ldquo;ug\u0026rdquo;] or [\u0026ldquo;pu\u0026rdquo;, \u0026ldquo;g\u0026rdquo;], depending on which of those segmentations is encountered first\nEach word in the corpus has a score, and the loss is the negative log likelihood of those scores — that is, the sum for all the words in the corpus of all the -log(P(word)).\nFinal Vocabulary: The resulting vocabulary consists of tokens that maximize the likelihood of the training data. This vocabulary is used to tokenize new text inputs for the language model\nLet’s go back to our example with the following corpus:\n(\u0026#34;hug\u0026#34;, 10), (\u0026#34;pug\u0026#34;, 5), (\u0026#34;pun\u0026#34;, 12), (\u0026#34;bun\u0026#34;, 4), (\u0026#34;hugs\u0026#34;, 5) The tokenization of each word with their respective scores is:\n\u0026#34;hug\u0026#34;: [\u0026#34;hug\u0026#34;] (score 0.071428) \u0026#34;pug\u0026#34;: [\u0026#34;pu\u0026#34;, \u0026#34;g\u0026#34;] (score 0.007710) \u0026#34;pun\u0026#34;: [\u0026#34;pu\u0026#34;, \u0026#34;n\u0026#34;] (score 0.006168) \u0026#34;bun\u0026#34;: [\u0026#34;bu\u0026#34;, \u0026#34;n\u0026#34;] (score 0.001451) \u0026#34;hugs\u0026#34;: [\u0026#34;hug\u0026#34;, \u0026#34;s\u0026#34;] (score 0.001701) So the loss is:\n10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8 Now we need to compute how removing each token affects the loss. removing \u0026ldquo;hug\u0026rdquo; will make the loss worse, because the tokenization of \u0026ldquo;hug\u0026rdquo; and \u0026ldquo;hugs\u0026rdquo; will become:\n\u0026#34;hug\u0026#34;: [\u0026#34;hu\u0026#34;, \u0026#34;g\u0026#34;] (score 0.006802) \u0026#34;hugs\u0026#34;: [\u0026#34;hu\u0026#34;, \u0026#34;gs\u0026#34;] (score 0.001701) These changes will cause the loss to rise by:\n- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5 Therefore, the token \u0026ldquo;pu\u0026rdquo; will probably be removed from the vocabulary, but not \u0026ldquo;hug\u0026rdquo;.\nImplement: Detail\nUnigram Encoding is a powerful tokenization method that leverages probabilistic modeling to optimize the token vocabulary for language models. Its simplicity, efficiency, and flexibility make it a valuable tool in NLP, helping to create robust and versatile language models. By understanding and applying Unigram Encoding, we can enhance the performance and adaptability of our language processing systems.\n2.4 SentencePiece Encoding All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, SentencePiece treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\nAdvantages: This approach is highly versatile, making it suitable for languages with complex word boundaries and noisy text data.\nApplication: SentencePiece is particularly useful in multilingual models and scenarios where the input text includes noise, such as OCR outputs or social media text.\n  Shervin Minaee et al. “Large Language Models: A Survey”. In: arXiv preprint arXiv:2402.06196 (2024).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nducthang.github.io/posts/2024-06-10-components-of-llm/","summary":"This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families\n1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.\n1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models.","title":"[LLM 02] Data cleaning and Tokenizations"},{"content":"Thank you all for visiting my blog. Today, I am excited to introduce some showcases demonstrating how my team and I have enhanced the image quality of Stable Diffusion. Additionally, I\u0026rsquo;m pleased to share that we have significantly improved Stable Diffusion\u0026rsquo;s text generation capabilities within images, achieving both beauty and stability.\nOur current research is being applied to develop an image-generating product that automatically creates ads from product URLs. Below are some of our showcases.\n   Image 1 Image 2              Image 3 Image 4             Image 5 Image 6             Image 7 Image 8           ","permalink":"https://nducthang.github.io/posts/2024-06-01-text-generation-in-sd/","summary":"Thank you all for visiting my blog. Today, I am excited to introduce some showcases demonstrating how my team and I have enhanced the image quality of Stable Diffusion. Additionally, I\u0026rsquo;m pleased to share that we have significantly improved Stable Diffusion\u0026rsquo;s text generation capabilities within images, achieving both beauty and stability.\nOur current research is being applied to develop an image-generating product that automatically creates ads from product URLs. Below are some of our showcases.","title":"My research aims to improve image and text quality in Stable Diffusion"},{"content":"This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.\n1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs. Based on Transformer, we can split neural networks in LLM into 3 categories:\n Encoder-Only Decoder-Only Encoder-Decoder  1.1 Encoder-Only The models only consist of an encoder network. Representative encode-only models include BERT and its variants (RoBERTa, DeBERTa, XLM, ALBERT, …)\n BERT (Bidirectional Encoder Representations from Transformers) is one of the most widely used encode-only. The pre-trained BERT can be fine-tuned for different tasks like classification, question answering…     BERT Model BERT Fine-tuning          BERT consists of 3 modules:\n Embedding module: Convert input text into a sequence of embedding vectors. Stack of Transformer encoders: Converts embedding vectors into contextual representation vectors. Fully connected layer: Converts the representation vector (at the final layer) to one-hot vectors.  BERT training with 2 objectives.\n Masked Language Modeling (MLM) Next Sentence prediction (NSP)   RoBERTa: improves the robustness of BERT by modifying a few key hyperparameters, removing NSP, and enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. ALBERT uses 2 parameter-reduction technologies to lower memory consumption and increase the training speed of BERT:  Splitting the embedding matrix into 2 smaller matrices. Using repeating layers split among groups.   DeBERTa (Decoding enhanced BERT with disentangled attention): improve BERT and RoBERTa use 2 novel techniques:  The disentangled attention mechanism: Each word is represented using 2 vectors that encode its content and position. An enhanced mask recorder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. l adversarial training method is used for fine-tuning to improve models’ generalization.   ELECTRAL: New pre-trained task - Replaced token detection (RTD): Instead of masking the input, RDT corrupts it by replacing some tokens with plausible alternatives sampled from a small generator. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not. XLM: Extended BERT to cross-lingual language models using 2 methods:  An Unsupervised method that only relies on monolingual data. A supervised method that leverages parallel data with a new cross-lingual language model objective.    1.2 Decoder-Only The most widely used decoder-only are GPT-1 and GPT-2, these models lay the foundation for more powerful LLMs subsequently (i.e. GPT-3, GPT-4).\n GPT-1: (Generative Pre-Training) is a decoder-only Transformer model on diverse corpus of unlabeled text in self-supervised learning fashion, followed by discriminative fine-tuning on each specific downstream tasks.   GPT-2: shows that language models are able to learn to perform specific natural language tasks without any explicit supervision when trained on a large WebText dataset consisting of millions of webpages. The GPT-2 model follows the model designs of GPT-1 with a few modifications:  Layer normalization is moved to the input of each sub-block, additional layer normalization is added after the final self-attention block. Context size is increased from 512 to 1024 tokens.    1.3 Encoder-Decoder The most widely used encoder-decoder are T5, mT5, MASS and BART.\n T5: Text-to-Text Transfer Transformer (T5) model where transfer learning is effectively exploited for NLP via an introduction of a unified framework in which all NLP tasks are cast as a text-to-text generation task. mT5: pre-trained on a newcommon Crawl based dataset consisting of texts in 101 languages. MASS: MAsked Sequence to Sequence pre-training: adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence. The encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and the decoder predicts the masked fragment. BART: sequence-to-sequence translation model architecture, is a pre-trained by corrupting text with an arbitrary nosing function, and then learning to reconstruct the original text.  2. Large Language Model Families In this section, I review 3 LLM families: GPT, LLaMA, PaLM. In addition to these models, there are other popular LLMs which do not belong to those three model families, yet they have achieved great performance and have pushed the LLMs field forward: FLAN, Gopher, T0, ERNIE 3.0, RETRO, GlaM, LaMDA, OPTChinchilla, Galactica, CodeGen, AlexaTM, Sparrow, Minerva, MoD, BLOOM, GLM, Pythia, Orca, StarCoder, Kosmos, Gemini,…\n2.1 GPT Family This family consists of GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, GPT-4, CODEX, and WebGPT, developed by OpenAI.\n GPT-3: is a pre-trained autoregressive language model with 175 million parameters. GPT-3 shows the emergent ability of in-context learning, which means GPT-3 can be applied to any downstream tasks without any gradient updates or fine-tuning. CodeX: is a general-purpose programming model that can parse natural language and generate code in response. Fine-tuned for programming applications on code corpora collected from GitHub. WebGPT: fine-tuned to answer open-ended questions using a text-based web browser, facilitating users to search and navigate the web. WebGPT is trained in 3 steps:  Firstly, WebGPT learn to mimic human browsing behaviors using human demonstration data. Secondly, a reward funtion is leared to predict human preferences. Finally, WebGPT is refined to optimize the reward function via reinforcemence learning and rejction sampling.   InstructGPT: is proposed to align language models with user intent on a wide range of tasks by fune-tuning with human feedback. The method is called Reinforcement Learning from Human Feedback (RLHF).  GPT-4: is a multimodal LLM in that it can take image and text as inputs and produce text outputs. GPT-4 was first pre-trained to predict next tokens on large text corpora, and then fine-tuned with RLHF to align model behaviors with human-desired ones.  2.2 LLaMA Family LLaMA is a collection of foundation language models, released by Meta. Unlike GPT models, LLaMA models are open-source.\n LLaMA: uses the transformer architecture of GPT-3, with a few minor architectural modifications, including:  Using a SwiGLU activation function instead of ReLU. Using rotary postional embeddings instead of absolute positional embedding. Using root-mean-squared layer normalization instead of standard layer nomialization.   LLaMA 2: include both foundation language models and Chat models funetuned for dialog (LLaMA-2 Chat)  Alpaca: is fune-tuned from the LLaMA-7B model using 52K instruction-following demonstations generated in style of self-instruct using GPT-3.5. Vicuna: fine-tuning LLaMA on user-shared conversations. Guanaco: also finetuned LLaMA models using instruction-following data. But the funetuning a 65B done a a single 48GB GPU with QLoRA. QLoRA back-propagates gradients through a frozen, 4-bit quantized pre-trained language model into Low Rank Adapters (LoRA). Mistral-7B: is a 7B parameter language model. . This model leverages grouped-query attention for faster inference, coupled with sliding window attention to effectively handle sequences of arbitrary length with a reduced inference cost.  2.3 PaLM Family The PaLM (Pathways Language Model) family are developed by Google. It is a 540B parameter transformer-based LLM, trained on high-quality text corpus consistaning of 780 billion tokens.\n U-PaLM: models of 8B, 62B, and 540B scales arecontinually trained on PaLM with UL2R, a method of continue training LLMs on a few steps with UL2’s mixture-of-denoiser objective. PaLM-2: is trained using a mixture of objectives. Through extensive evaluations on English, multilingual, and reasoning tasks. Med-PaLM: is a medical questions domainm finetuned on PaLM using instruction prompt tuning, a parameter-effectient method for aligning LLMs to new domains using a few examples.  3. How LLMs Are Built? To build a large language model (LLM) from scratch, the process generally involves the following stages:\n  Data Cleaning: In this stage, we clean the data by filtering out duplicates, removing noise, and preprocessing the input data to ensure quality and consistency.\n  Tokenization: Tokenization is the process of breaking down text into smaller units called tokens, which can be words, phrases, symbols, or other entities, depending on the use case and processing method. Various tokenization methods include WordPiece Encoding, SentencePiece Encoding, and Byte Pair Encoding.\n  Positional Encoding: This technique is used in NLP models to provide information about the positions of words in a sequence. It is especially crucial for non-sequential models like Transformers. Since Transformers do not process data in a sequential manner like RNNs or LSTMs, positional encoding is necessary to encode the positional information of words in a sequence.\n  Choosing LLM Architectures: Selecting the architecture for the LLM, such as Encoder-Only, Decoder-Only, or Encoder-Decoder models.\n  Model Pre-training: Pre-training the model using various methods, depending on the goals and architecture of the model, such as Masked Language Modeling, Causal Language Modeling, Next Sentence Prediction, and Mixture of Experts.\n  Fine-tuning and Instruction Tuning: This involves adjusting a pre-trained model for a specific task by training it further on a new dataset related to that task. Common methods for fine-tuning and instruction tuning include Supervised Fine-Tuning, General Fine-Tuning, Multi-Turn Instructions, and Instruction Following.\n  Alignment: Ensuring that the behavior of AI models aligns with the intentions and goals of users or developers. There are two main types of alignment: human alignment and task-specific alignment. Popular methods include Supervised Learning, Reinforcement Learning from Human Feedback (RLHF), Direct Preference Optimization, and KT Optimization.\n  Decoding Strategies: These are methods used to generate text sequences from the language model after it has been trained. The goal is to produce coherent and contextually appropriate text. Common strategies include Greedy Search, Beam Search, Top-K Sampling, and Top-P Sampling.\n  Cost-Effective Training/Inference, Adaptation \u0026amp; Compression: Optimizing costs and improving the efficiency of training and inference in LLMs, ensuring the models are scalable and deployable in cost-effective ways.\n  This comprehensive process ensures that the LLM is robust, efficient, and aligned with its intended use, delivering high-quality, coherent, and contextually appropriate text outputs.\nWe will explore each component in detail in the following articles.\n  Shervin Minaee et al. “Large Language Models: A Survey”. In: arXiv preprint arXiv:2402.06196 (2024).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/","summary":"This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.\n1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs.","title":"[LLM 01] Large Language Model Overview"},{"content":"    I strongly encourage you to explore my earlier article, Understanding the Influence Function, as it serves as a valuable foundation for comprehending the content presented in this piece.\n What is error detection problem? The rapid growth of the internet, however, causes data to rise exponentially, posing numerous issues. Deep learning algorithms become less effective when big data is mislabeled or contains many errors. Current studies focus solely on improving the model rather than detecting data issues. Research on error tracing has not achieved high accuracy and is costly to store and compute, and is even void of stability or dependent on many different constraints and assumptions.\nThe model will work more accurately if the dataset is improved. Therefore, we concentrate on seeking solutions for resolving the problem of error tracing. We have a data set used to train the model, but it is enormous or requires a lot of expertise (such as in medical and engineering sectors); thus, people cannot manually check it. Besides, we will not be able to figure out how many incorrect patterns there are. Error detection problem to identify wrong data patterns in the original data set that we can modify to help improve the performance of deep learning models. The method in error dection problem must have high accuracy and fast access speed, minimize dependence on theoretical hypotheses, and be generalizable, suited for a wide range of data and deep learning models.\nIn this article, I will overview an approach to the error detection problem which is gradient-based methods, of which the Influence funtion in my previous article is one of those methods.\nRepresenter Points Representer Points 1 introduced by Yeh et al to approximate the influence function of training points on a test sample. To indicate the form of a representer theorem, suppose we solve for the optimal parameters: $$ \\theta^{*} = \\underset{\\theta \\in \\Theta}{\\text{argmin}} \\left\\{ \\frac{1}{n} \\sum_i^n L(x_i,y_i, \\theta) + g(\\| \\theta \\|) \\right\\} $$ where \\(g\\) is non-decreasing, \\(L_2\\) regularized. The parameterization spliting of the model as a feature model and a prediction network:  Feature model: \\(\\phi_2(x_i, \\theta_2)\\) can be arbitrarily deep, or simply the identify function. Prediction network: with parameters \\(\\theta_1\\) is \\(\\phi(x_i, \\theta) = \\theta_1 f_i \\subseteq \\mathbb{R}^c\\) and \\(f_i = \\phi_2 (x_i, \\theta_2) \\subseteq \\mathbb{R}^f\\) is the last intermedicate layer feature in the neural network for input \\(x_i\\). So, we have a prediction of model is \\(\\hat{y}_i = \\sigma\\left( \\phi(x_i, \\theta) \\right)\\) and let \\(g(\\| \\theta \\|) = \\lambda \\| \\theta_1 \\|^2\\). Then, we have the decomposition: $$ \\phi(x_t, \\theta^{*}) = \\sum_{i}^n k(x_t, x_i, \\alpha_i) $$ where \\(\\alpha_{i}=\\frac{1}{-2 \\lambda{n}} \\frac{\\partial \\mathcal{L}\\left(x_{i}, y_{i}, \\theta\\right)}{\\partial \\phi\\left(x_{i}, \\theta\\right)}\\) and \\(k(x_t, x_i, \\alpha_i)=\\alpha_i f_i^T f_t\\), which we call a representer value for \\(x_i\\) given \\(x_t\\). We showed that for such models the output for any target instance \\(x_t\\) can be expressed as a linear decomposition of “data importance” of training instances. Through this, we can evaluate when a testing point of the model predicts incorrectly which training points affect it the most, and expect that those training points are mislabeled data.   Proof: Note that for any stationary point, the gradient of the loss with respect to \\(\\theta_1\\) is equal to \\(0\\). We therefore have: $$ \\frac{1}{n} \\sum_{i=1}^{n} \\frac{\\partial L\\left({x}_{i}, {y}_{i}, {\\theta}\\right)}{\\partial {\\theta}_{1}}+2 \\lambda {\\theta}_{1}^{*}=0 \\quad \\Rightarrow \\quad {\\theta}_{1}^{*}=-\\frac{1}{2 \\lambda n} \\sum_{i=1}^{n} \\frac{\\partial L\\left({x}_{i}, {y}_{i}, {\\theta}\\right)}{\\partial {\\theta}_{1}}=\\sum_{i=1}^{n} \\alpha_{i} {f}_{i}^{T} $$ where \\(\\alpha_{i}=\\frac{1}{-2 \\lambda{n}} \\frac{\\partial \\mathcal{L}\\left(x_{i}, y_{i}, \\theta\\right)}{\\partial \\phi\\left(x_{i}, \\theta\\right)}\\) by the chain rule. We thus have that: $$ \\phi(x_t, \\theta^{*}) = \\theta_1^{*} f_t = \\sum_{i}^n k(x_t, x_i, \\alpha) $$ where \\(k(x_t, x_i, \\alpha_i) = \\alpha_i f_i^T f_t\\) by simply plugging in the two above expression. Gradient Dot and Gradient Cosine Similarity These methods are introduced by Charpiat et al 2. Basically, these methods are based on influence function. If one wants to change the value of \\(L(z, \\theta)\\) by a small quantity \\(\\epsilon\\), one needs to update \\(\\theta\\) by \\(\\delta \\theta = \\epsilon \\frac{\\nabla_{\\theta}L(z, \\theta)}{\\| \\nabla_{\\theta} L(z, \\theta) \\|^2}\\). Indeed, after the parameter update, the new value at \\(z\\) will be: $$ L(z, \\theta + \\delta \\theta)(z) = L(z, \\theta) + \\nabla L(z, \\theta) \\cdot \\delta \\theta + O( \\| \\delta \\theta \\|) \\\\ = L(z, \\theta) + \\epsilon + O(\\epsilon^2) $$ This parameter change induces a value change at any other point \\(z'\\): $$ L(z', \\theta + \\delta \\theta) = L(z', \\theta) + \\nabla L(z', \\theta) \\cdot \\delta \\theta +O(\\| \\delta \\theta \\|) \\\\ = L(z', \\theta) + \\epsilon \\frac{\\nabla L(z', \\theta) \\cdot \\nabla L(z, \\theta)}{\\| \\nabla L(z, \\theta) \\|^2 } + O(\\epsilon^2) $$ Therefore the kernel represents the influence of $z$ over $z'$: $$ k_{\\theta}^N (z, z') = \\frac{\\nabla L(z', \\theta) \\cdot \\nabla L(z, \\theta)}{\\| \\nabla L(z, \\theta) \\|^2 } $$ Note however that \\(k_{\\theta}^N (x, x')\\) is not symmetric. We have two symmetric kernels natural arise:  The inner product: (We called it Gradient Dot) $$ k_{\\theta}^I (z, z') = \\nabla_{\\theta} L(z, \\theta) \\cdot \\nabla L(z', \\theta) $$  Normalized version: (We called it Gradient Cosine Similarity) $$ k_{\\theta}^C (z, z') = \\frac{\\nabla L(z, \\theta)}{\\|\\nabla L(z, \\theta) \\| } \\cdot \\frac{\\nabla L(z',\\theta)}{\\| \\nabla L(z', \\theta) \\|} $$ It can be seen that \\(k_{\\theta}^C (z, z')\\) has the advantage of being bounded (in \\([-1, 1]\\), thus expressing similarity in a usual meaning. Interestingly, \\(k_{\\theta}^I\\) is equivalent to the Influence function without Hessian, in An Empirical Comparison of Instance Attribution Methods for NLP 3 experimented and concluded that this formula is roughly equivalent to influence function with Hessian.  RelatIF method One shortcoming of influence functions is that the training examples deemed most influential are ofter outliers or mislabeled, making them poor choices for an explanation. RelatIF 4 is a new class of criteria for choosing relevant training examples by way of an optimization objective that places a constraint on global influence. Above figure described binary classification by linear decision boundaries (dashed line) to illustrate the difference between IF and RelatIF.\nIn figure (a), we can see that the model predicts the test input (star). As estimated by IF, the most influential training example for the prediction is an outlier (circled). Using RelatIF, the most effective training example is more typical (encased in a square).\nFigure (b) see that using IF, every test input falling within the shaded yellow region is most influenced by the same outlier (circled). Test inputs in the remaining white area are most influenced by one of 5 other high loss examples.\nFigure (c) using RelatIF, the area where test inputs are most affected by the outlier (circled) shrinks. Test inputs in the remaining region are most influenced by one of 65 other examples. To evaluate the effect of a training example \\(z_i\\) on testing example \\(z_{\\text{test}}\\), we have: $$ \\textbf{RelatIF}(z_i, z_{\\text{test}}) = \\cos \\left(H^{-\\frac{1}{2}} \\nabla_{\\theta} L(z_{\\text{test}}), H^{-\\frac{1}{2}} \\nabla_{\\theta} L(z_i) \\right) $$ where \\(\\cos\\) is cosine similarity function. TracIn method Introduction TracIn method 5 introduced by Garima and Freferick Liu et al. It computes the influence of a training example on a prediction made by the model. It applies to any machine learning model trained using stochastic gradient descent or variant, agnostic architecture, domain, and task.\nThe idealized notion of the influence of a particular training example \\(z\\) in a given test example \\(z'\\) is defined as the total reduction in loss on the test example \\(z'\\) that is induced by the training process whenever the training example \\(z\\) is utilized: $$ \\text{TracInIdeal(z,z')} = \\sum_{t:z_t =z} L(z', \\theta_t) - L(z', \\theta_{t+1}) $$ where \\(\\theta_t\\) and \\(\\theta_{t+1}\\) is parameters of model when using training example \\(z_t\\) with SGD to updating from \\(\\theta_t\\) to \\(\\theta_{t+1}\\). Suppose the initial parameter vector before starting the training process is \\(\\theta_0\\), and the final parameter vector is \\(\\theta_T\\). The iterative optimization technique operates on one training example at a time. Then: $$ \\sum_{i=1}^n \\text{TracInIdeal}(z_i,z') = L(z', \\theta_0) - L(z', \\theta_T) $$ Let:\n Proponents: Training samples that have positive value to influence score (reducing loss). Or so-called helpful/excitatory samples. Opponents: Training samples that have negative value to influence score (increase loss). Or so-called harmful/inhibitor samples   First-order Approximation to Idealized Influence We can approximate the change in the loss: $$ L(z',\\theta_{t+1}) = L(z',\\theta_t) + \\nabla L(z', \\theta_t) \\cdot (\\theta_{t+1} - \\theta_t) + O(\\|\\theta_{t+1} - \\theta_t \\|^2) $$ Using the training point \\(z_t\\) at iteration \\(t\\), then the change in parameters: $$ \\theta_{t+1} - \\theta_t = -\\eta_t \\nabla L(z_t, \\theta_t) $$ where \\(\\eta_t\\) is the step size in iteration \\(t\\). This formula should be changed appropriately if other optimization methods as AdaGrad, Adam, Newton,\\(\\ldots\\\\\\) Combining 2 above equations, we have: $$ L(z', \\theta_{t}) - L(z', \\theta_{t+1}) \\approx \\eta_t \\nabla L(z',\\theta_t) \\cdot (\\theta_{t+1} - \\theta_t) $$ The effect can be approximated for a training sample $z$ by summing the approximations over all iterations, where \\(z\\) is used to update the parameter. $$ \\text{TracIn}(z, z') = \\sum_{t:z_t=z} \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t) $$ Extension to Minibatches To handle minibatches of size \\(b \\ge 1\\), we compute the influence of a minibatch on the test point \\(z'\\), and then take its first-order approximation: First-Order Approximation \\((B_t, z') = \\frac{1}{b} \\sum_{z \\in B_t} \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t) \\), because the gradient for the minibatch \\(B_t\\) is \\(\\frac{1}{b} \\sum_{z \\in B_t} \\nabla L(z, \\theta_t)\\). Then, for each training point \\(z \\in B_t\\), we attribute the \\(\\frac{1}{b} \\cdot \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t)\\) portion of the influence of \\(B_t\\) on the test point \\(z'\\). Summing up over all iteratios \\(t\\) in which a particular training point \\(z\\) was chosen in \\(B_t\\), we arrive at the following definition of TracIn with minibatches: $$ \\text{TracIn}(z, z') = \\frac{1}{b} \\sum_{t:z \\in B_t} \\eta_t \\nabla L(z', \\theta_t) \\cdot \\nabla L(z, \\theta_t) $$ TracIn with Checkpoint Suppose we have \\(k\\) checkpoints \\(\\theta_{t_1}, \\theta_{t_2},\\ldots,\\theta_{t_k}\\). We assume that between checkpoints, each training example is visited exactly once. And we use the notation \\(\\eta_i\\) to denote the step size between checkpoints \\(i-1\\) and \\(i\\). While the first-order approximation of the influence needs the parameter vector at the specific iteration where a given training example is visited, since we don't have access to the parameter vector, we approximate it with the first checkpoint parameter vector after it. Thus, this heuristic results: $$ \\text{TracInCP}(z,z') = \\sum_{i=1}^k \\eta_i \\nabla L(z, \\theta_{t_i}) \\cdot \\nabla L(z', \\theta_{t_i}) $$ Summary Through this article, I have introduced you to gradient-based methods employed in error detection tasks. While fundamentally providing satisfactory results, these gradient-based approaches demand significant computational resources and storage capacity.\n  Chih-Kuan Yeh et al. “Representer point selection for explaining deep neural networks”. In: arXiv preprint arXiv:1811.09720 (2018).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Guillaume Charpiat et al. “Input similarity from the neural network perspective”. In: NeurIPS 2019-33th Annual Conference on Neural Information Processing Systems. 2019.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Pouya Pezeshkpour et al. “An Empirical Comparison of Instance Attribution Methods for NLP”. In: arXiv preprint arXiv:2104.04128 (2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Elnaz Barshan, Marc-Etienne Brunet, and Gintare Karolina Dziugaite. “Relatif:Identifying explanatory training samples via relative influence”. In: International Conference on Artificial Intelligence and Statistics. PMLR. 2020, pp. 1899–1909.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Garima Pruthi et al. “Estimating training data influence by tracing gradient descent”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 19920–19930.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nducthang.github.io/posts/2023-11-25-gradient-based-methods/","summary":"I strongly encourage you to explore my earlier article, Understanding the Influence Function, as it serves as a valuable foundation for comprehending the content presented in this piece.\n What is error detection problem? The rapid growth of the internet, however, causes data to rise exponentially, posing numerous issues. Deep learning algorithms become less effective when big data is mislabeled or contains many errors. Current studies focus solely on improving the model rather than detecting data issues.","title":"Gradient-based methods in error detection"},{"content":"   In this article, I review about Influence functions and various of its - a classic technique from robust statistics - to trace a model\u0026rsquo;s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction.\nBasics of influence function Consider a prediction problem from some input space \\(\\mathcal{X}\\) (e.g., images, text,\\(\\ldots\\)) to an output space \\(\\mathcal{Y}\\) (e.g,. labels). We are given training points \\(z_1,\\ldots,z_n\\), where \\(z_i=(x_i,y_i) \\in \\mathcal{X} \\times \\mathcal{Y}\\). For a point \\(z\\) and parameters \\(\\theta \\in \\Theta\\), let \\(L(z, \\theta)\\) be the loss, and let \\(\\frac{1}{n} \\sum_{i=1}^n L(z_i, \\theta)\\) be the empirical risk. The empirical risk minimizer is given by $$ \\hat{\\theta} \\stackrel{\\text { def }}{=} \\arg \\min _{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L\\left(z_{i}, \\theta\\right) $$ Assume that the empirical risk is twice-differentiable and strictly convex in \\(\\theta\\). Our goal is to understand the effect of training points on a model's predictions. Up-weighting a training point  Up-weighting a training example \\(z\\) by an infinitesimal amount \\(\\epsilon\\) leads to a new set of model parameters denoted by \\(\\hat{\\theta}_{\\epsilon, z}\\). This set of new model parameters \\(\\hat{\\theta}_{\\epsilon, z}\\) is obtained by solving: $$ \\hat{\\theta}_{\\epsilon, z} \\stackrel{\\text { def }}{=} \\arg \\min _{\\theta \\in \\Theta} \\frac{1}{n} \\sum_{i=1}^{n} L\\left(z_{i}, \\theta\\right)+\\epsilon L(z, \\theta) $$ Removing a training point \\(z\\) is similar to up-weighting its corresponding weight by \\(\\epsilon = \\frac{-1}{n}\\) in below equation. The main idea used is to approximate \\(\\hat{\\theta}_{\\epsilon, z}\\) by the first-order Taylor series expansion around the optimal model parameters represented by \\(\\theta^{*}\\), which leads to: $$ \\hat{\\theta}_{\\epsilon, z} \\approx \\theta^{*} - \\epsilon H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta}) $$ where \\(H_{\\hat{\\theta}} \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\theta}^{2} L\\left(z_{i}, \\hat{\\theta}\\right)\\) is the Hessian and is positive definite (PD) by assumption. Following the result in Pang Wei Koh and Percy Liang1, the change in the model parameters (\\(\\Delta\\theta = \\hat{\\theta}_{-z} - \\hat{\\theta} \\approx - \\frac{1}{n} \\mathcal{I}_{\\text {up,params }} (z)\\): $$ \\left.\\mathcal{I}_{\\text {up,params }}(z) \\stackrel{\\text { def }}{=} \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0}=-H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta}) $$ The change in the loss value for a particular test point \\(z_t\\) when a training point \\(z\\) is up-weighted can be approximated as a closed-form expression by the chain rule: $$ \\begin{aligned} \\mathcal{I}_{\\text {up,loss }}\\left(z, z_{\\text {test }}\\right) \u0026\\left.\\stackrel{\\text { def }}{=} \\frac{d L\\left(z_{\\text {test }}, \\hat{\\theta}_{\\epsilon, z}\\right)}{d \\epsilon}\\right|_{\\epsilon=0} \\\\ \u0026=\\left.\\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0} \\\\ \u0026=-\\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)^{\\top} H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L(z, \\hat{\\theta}) . \\end{aligned} $$ Below equation is approximately the change in the loss for the test-sample \\(z_{\\text{test}}\\) when a training sample \\(z\\) is removed from the training set. In addition, in the paper 1, the authors also introduce Perturbing a training input. However, it is less common than up-weighting a training point.\nDeriving the influence function \\(\\mathcal{I}_{\\text{up, params}}\\) Recall that \\(\\hat{\\theta}\\) minimizes the empirical risk: $$ R(\\theta) \\stackrel{\\text { def }}{=} \\frac{1}{n} \\sum_{i=1}^{n} L\\left(z_{i}, \\theta\\right) $$ We further assume that \\(R\\) is twice-differentable and strongly convex in \\(\\theta\\), i.e: $$ H_{\\hat{\\theta}} \\stackrel{\\text { def }}{=} \\nabla^{2} R(\\hat{\\theta})=\\frac{1}{n} \\sum_{i=1}^{n} \\nabla_{\\theta}^{2} L\\left(z_{i}, \\hat{\\theta}\\right) $$ exits and is positive definite. This guarantees the existence of \\(H_{\\hat{\\theta}}^{-1}\\), which we will use in the subsequent derivation. The perturbed parameters \\(\\hat{\\theta}_{\\epsilon, z}\\) can be written as: $$ \\hat{\\theta}_{\\epsilon, z}=\\arg \\min _{\\theta \\in \\Theta}\\{R(\\theta)+\\epsilon L(z, \\theta)\\} $$ Define the parameter change \\(\\Delta_{\\epsilon} = \\hat{\\theta}_{\\epsilon,z} - \\hat{\\theta}\\), and note that, as \\(\\hat{\\theta}\\) doesn't depend on \\(\\epsilon\\), the quantity we seek to compute can be written in terms of it: $$ \\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}=\\frac{d \\Delta_{\\epsilon}}{d \\epsilon} $$ Since \\(\\hat{\\theta}_{\\epsilon, z}\\) is a minimizer of above equation, let us examine its first-order optimality conditions: $$ 0=\\nabla R\\left(\\hat{\\theta}_{\\epsilon, z}\\right)+\\epsilon \\nabla L\\left(z, \\hat{\\theta}_{\\epsilon, z}\\right) $$ Next, since \\(\\hat{\\theta}_{\\epsilon, z} \\to \\hat{\\theta}\\) as \\(\\epsilon \\to 0\\), we perform a Taylor expansion of the right-band side: $$ \\begin{aligned} 0 \\approx \u0026[\\nabla R(\\hat{\\theta})+\\epsilon \\nabla L(z, \\hat{\\theta})]+\\\\ \u0026\\left[\\nabla^{2} R(\\hat{\\theta})+\\epsilon \\nabla^{2} L(z, \\hat{\\theta})\\right] \\Delta_{\\epsilon} \\end{aligned} $$ where we have dropped \\(o\\left(\\left\\|\\Delta_{\\epsilon}\\right\\|\\right)\\) terms. Solving for \\(\\Delta_{\\epsilon}\\), we get: $$ \\begin{array}{c} \\Delta_{\\epsilon} \\approx-\\left[\\nabla^{2} R(\\hat{\\theta})+\\epsilon \\nabla^{2} L(z, \\hat{\\theta})\\right]^{-1} \\\\ {[\\nabla R(\\hat{\\theta})+\\epsilon \\nabla L(z, \\hat{\\theta})]} \\end{array} $$ since \\(\\hat{\\theta}\\) minimizes \\(R\\), we have \\(\\nabla R(\\hat{\\theta})=0\\). Dropping \\(o(\\epsilon)\\) terms, we have: $$ \\Delta_{\\epsilon} \\approx-\\nabla^{2} R(\\hat{\\theta})^{-1} \\nabla L(z, \\hat{\\theta}) \\epsilon $$ So, we conclude that: $$ \\begin{aligned} \\left.\\frac{d \\hat{\\theta}_{\\epsilon, z}}{d \\epsilon}\\right|_{\\epsilon=0} \u0026=-H_{\\hat{\\theta}}^{-1} \\nabla L(z, \\hat{\\theta}) \\\\ \u0026 \\stackrel{\\text { def }}{=} \\mathcal{I}_{\\text {up,params }}(z) \\end{aligned} $$ Efficiently calculating influence Hessian-vector products  It is not easy and expensive to compute the matrix \\(H_{\\theta}^{-1}\\) directly. We can use Hessian-vector products (HVPs) to efficiently approximate \\(s_{\\text {test }} \\stackrel{\\text { def }}{=} H_{\\hat{\\theta}}^{-1} \\nabla_{\\theta} L\\left(z_{\\text {test }}, \\hat{\\theta}\\right)\\) and then compute \\(\\mathcal{I}_{\\text {up,loss }}\\left(z, z_{\\text {test }}\\right)=-s_{\\text {test }} \\cdot \\nabla_{\\theta} L(z, \\hat{\\theta})\\). With \\(s_{\\text{test}}\\), we can recursively compute the following: $$ \\left\\{ \\begin{matrix} H^{-1}_{0} v = v \\\\ H_j^{-1}v = v + (I-\\nabla^2_{\\theta} L(z_{s_j}, \\hat{\\theta})) H_{j-1}^{-1}v \\end{matrix} \\right. $$ Initialization: \\(v = \\nabla_{\\theta} L(z_{test}, \\hat{\\theta})\\), the \\(z_{s_j}\\) points are randomly taken from the training set. Equation equivalent: $$ \\left\\{ \\begin{matrix} H^{-1}_{0} v = v \\\\ H_j^{-1}v = v + IH_{j-1}^{-1}v - \\nabla^2_{\\theta} L(z_{s_j}, \\hat{\\theta}) H_{j-1}^{-1}v \\end{matrix} \\right. $$ The third component of the above equation is equivalent to calculating \\(\\mathbf{Hv}\\) where: \\(\\mathbf{H}=\\nabla^2_{\\theta} L(z_{s_j}, \\hat{\\theta})\\) and \\(\\mathbf{v}=H_{j-1}^{-1}v\\). In Heterogeneous uncertainty sampling for supervised learning2, we have: $$ \\mathbf{H} \\mathbf{v}=\\nabla_{\\mathbf{\\theta}}\\left(\\mathbf{v} \\cdot \\nabla_{\\mathbf{\\theta}} L\\right) $$ In influence function calculated as above, cost and approximation quality depends:\n J: the number of recursive iterations. T: the number of independent runs. B: the batch size of sample points from training data (Number of \\(z_{s_j}\\) ). To improve the speed of influence function, the paper Fastif: Scalable influence functions for efficient model interpretation and debugging 3 suggest some ideas such as: Speeding up the argmax using kNN, Speeding up the Inverse Hessian, Parallelization.  Speeding up the argmax using kNN  With the influence function, we need to calculate influence on the full training data. $$ z^{*}=\\underset{z \\in \\mathcal{Z}}{\\arg \\max } \\mathcal{I}\\left(z, z_{\\text {test }}\\right) $$ We hypothesize that we could constrain the expensive search to a subset of promising data points, \\(\\hat{\\mathcal{Z}} \\subseteq \\mathcal{Z}\\): $$ z^{*}=\\underset{z \\in \\hat{\\mathcal{Z}}}{\\arg \\max } \\mathcal{I}\\left(z, z_{\\text {test }}\\right) $$ We can select subset \\(\\hat{\\mathcal{Z}}\\) as the top-k nearest neighbors of \\(z_{\\text{test}}\\) based on the \\(L_{2}\\) distance between extracted features of the data-points and can use libraries such as FAISS 4.\nTo evaluate, define the recall score R@m as the percentage of top-m ground-truth influential data points selected by the kNN. $$ R @ m=\\frac{\\mid\\{\\text { retrieved }\\} \\cap\\{\\text { top- } m \\text { influential }\\} \\mid}{\\mid\\{\\text { top- } m \\text { influential } \\mid} $$ Speeding up the Inverse Hessian Propose a few simple changes:\n Choose a J so that approximation converges. Choose a small batch size. In our experiments, we found that even B = 1 suffices. Make up for the noisiness of small batch size using larger T , which can be distributed over multiple GPUs.  Parallelization We can apply asynchronous parallel computation to compute the \\(s_{\\text{test}}\\), use one synchronization point to average the result using all-reduce, and then asynchronously compute the influence of a subset of data-points that are pre-selected using kNN Influence functions in deep learning are fragile The influence function method is unstable in deep learning networks. According to the theory of influence functions, they work well with convex functions. However, it is not clear for non-convex functions. In the paper Influence functions in deep learning are fragile 5 provides comparative experiments on influence functions in networks with different depths and with different data. Experimentally, the authors concluded that:\n Influence function estimates fairly accurately when networks are shallow, not so well in deeper networks. Train model with weight decay \\(\\lambda\\| \\theta \\|_2^2\\) into the loss function to increase accuracy. The accuracy of influence can vary considerably depending on the test points.  Based on theory and experiment, I see that the influence function does not work effectively with non-convex optimization problems. This is also the biggest drawback of this method.\n  Pang Wei Koh and Percy Liang. “Understanding black-box predictions via influence functions”. In: International Conference on Machine Learning.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n David D Lewis and Jason Catlett. “Heterogeneous uncertainty sampling for supervised learning”. In: Machine learning proceedings 1994. Elsevier, 1994, pp. 148–156.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Han Guo et al. “Fastif: Scalable influence functions for efficient model interpretation and debugging”. In: arXiv preprint arXiv:2012.15781 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Jeff Johnson, Matthijs Douze, and Herve Jegou. “Billion-scale similarity searchwith gpus”. In: IEEE Transactions on Big Data (2019).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Samyadeep Basu, Philip Pope, and Soheil Feizi. “Influence functions in deep learning are fragile”. In: arXiv preprint arXiv:2006.14651 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://nducthang.github.io/posts/2023-11-24-influence-function/","summary":"In this article, I review about Influence functions and various of its - a classic technique from robust statistics - to trace a model\u0026rsquo;s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction.\nBasics of influence function Consider a prediction problem from some input space \\(\\mathcal{X}\\) (e.g., images, text,\\(\\ldots\\)) to an output space \\(\\mathcal{Y}\\) (e.g,. labels).","title":"What is Influence Function?"}]