<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>[LLM 02] Data cleaning and Tokenizations | Thang's Blog</title><meta name=keywords content="LLM"><meta name=description content="This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families
1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.
1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models."><meta name=author content="Thang Nguyen-Duc"><link rel=canonical href=https://nducthang.github.io/posts/2024-06-10-components-of-llm/><link crossorigin=anonymous href=/assets/css/stylesheet.19ce2e35b67c1eb3628e21b8844ca2f72d2adf9d9c715e7e568a40f209923876.css integrity="sha256-Gc4uNbZ8HrNijiG4hEyi9y0q352ccV5+VopA8gmSOHY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nducthang.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://nducthang.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://nducthang.github.io/icon.png><link rel=apple-touch-icon href=https://nducthang.github.io/icon.png><link rel=mask-icon href=https://nducthang.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-MZBM3J5EX7"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-MZBM3J5EX7",{anonymize_ip:!1})}</script><meta property="og:title" content="[LLM 02] Data cleaning and Tokenizations"><meta property="og:description" content="This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families
1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.
1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models."><meta property="og:type" content="article"><meta property="og:url" content="https://nducthang.github.io/posts/2024-06-10-components-of-llm/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-06-10T00:00:00+00:00"><meta property="article:modified_time" content="2024-06-10T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[LLM 02] Data cleaning and Tokenizations"><meta name=twitter:description content="This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families
1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.
1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nducthang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"[LLM 02] Data cleaning and Tokenizations","item":"https://nducthang.github.io/posts/2024-06-10-components-of-llm/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[LLM 02] Data cleaning and Tokenizations","name":"[LLM 02] Data cleaning and Tokenizations","description":"This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families\n1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.\n1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models.","keywords":["LLM"],"articleBody":"This article summarizes word knowledge from Large Language Models: A Survey 1 . This is a series of articles continuing LLM 01 - Large Language Model Families\n1. Data Cleaning Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.\n1.1 Data Filtering Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models. Common data filtering techniques include:\n  Removing Noise: This involves eliminating irrelevant or noisy data that might impair the model’s ability to generalize. For instance, removing false information from the training data can reduce the likelihood of the model generating incorrect responses. Two mainstream approaches for quality filtering are classifier-based and heuristic-based frameworks.\n  Handling Outliers: Identifying and managing outliers or anomalies in the data to prevent them from disproportionately influencing the model. This ensures that the model learns from typical data patterns rather than skewed anomalies.\n  Addressing Imbalances: Balancing the distribution of classes or categories in the dataset to avoid biases and ensure fair representation. This is particularly important for responsible model training and evaluation.\n  Text Preprocessing: Cleaning and standardizing text data by removing stop words, punctuation, or other elements that may not contribute significantly to the model’s learning. This step ensures that the data is uniform and free of unnecessary noise.\n  Dealing with Ambiguities: Resolving or excluding ambiguous or contradictory data that might confuse the model during training. By clarifying these ambiguities, the model can provide more definite and reliable answers.\n  1.2 Deduplication Deduplication refers to the process of removing duplicate instances or repeated occurrences of the same data in a dataset. Duplicate data points can introduce biases in the model training process and reduce data diversity, as the model may overfit on those particular instances.\n1.3 Summary In summary, data cleaning, which includes both filtering and deduplication, is essential for improving the performance of language models. By ensuring high-quality, diverse, and representative training data, we can train more effective and reliable models. As demonstrated by the Falcon40B example, rigorous data cleaning can lead to significant advancements in model performance, highlighting the importance of these techniques in the development of large language models.\n2. Tokenizations Tokenization refers to the process of converting a sequence of text into smaller parts known as tokens. While the simplest tokenization tool merely splits text based on white space, more advanced tokenization tools rely on a word dictionary. However, this approach faces the challenge of out-of-vocabulary (OOV) words, as the tokenizer can only recognize words within its dictionary. To mitigate this issue and improve dictionary coverage, popular tokenizers for large language models (LLMs) are based on sub-words. These sub-words can be combined to form a large number of words, including those not seen during training or those in different languages. Here, we will explore three widely used tokenization methods: Byte Pair Encoding, WordPiece Encoding, and SentencePiece Encoding.\n2.1 Byte Pair Encoding (BPE) Byte Pair Encoding is a algorithm that uses frequent patterns at byte level to compress data. For example, Our corpus contain words:\nlow low low lower So, we have base vocabulary is:\n ['l', 'o', 'w', 'e', 'r'] After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one. At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by “pair,” here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step. Iterations:\nVocabulary: ['l', 'o', 'w', 'e', 'r'] Corpus: [('l', 'o', 'w', 3), ('l', 'o', 'w', 'e', 'r', 1)] Then we look at pairs. The pair (’l’, ‘o’) is present in the word low and lower with total 4 times. So, we have:\nVocabulary: ['l', 'o', 'w', 'e', 'r', 'lo'] Corpus: [('lo', 'w', 3), ('lo', 'w', 'e', 'r', 1)] Then:\nVocabulary: ['l', 'o', 'w', 'e', 'r', 'lo', 'low'] Corpus: [('low', 3), ('low', 'e', 'r', 1)] And we continue like this until we reach the desired vocabulary size.\nImplement: Detail\nAdvantages: BPE is effective in representing morphological variations of frequent words, as common prefixes and suffixes are well captured if they frequently appear in the training data.\nApplication: BPE is widely used in models where efficient vocabulary management and flexibility to handle new words are crucial.\n2.2 Word Piece Encoding WordPiece Encoding is primarily used in well-known models like BERT and Electra. It aims to address the issue of unknown tokens (UNK) by ensuring all characters in the training data are represented in the vocabulary.\nWordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, “word” gets split like this:\nw ##o ##r ##d Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix. Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula: $$score=(freq\\_of\\_pair)/(freq\\_of\\_first\\_element \\times freq\\_of\\_second\\_element)$$ By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.\nExample: We have vocabulary and frequent of each words as follow:\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) The most frequent pair is (\"##u\", “##g”) (present 20 times), but the individual frequency of “##u” is very high, so its score is not the highest (it’s 1 / 36). All pairs with a “##u” actually have that same score (1 / 36), so the best score goes to the pair (\"##g\", “##s”) — the only one without a “##u” — at 1 / 20, and the first merge learned is (\"##g\", “##s”) - (\"##gs\").\nVocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\"] Corpus: (\"h\" \"##u\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"h\" \"##u\" \"##gs\", 5) Iterative: (“h”, “##u”) - “hu”.\nVocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\"] Corpus: (\"hu\" \"##g\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5) Then the next best score is shared by (“hu”, “##g”) and (“hu”, “##gs”) (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:\nVocabulary: [\"b\", \"h\", \"p\", \"##g\", \"##n\", \"##s\", \"##u\", \"##gs\", \"hu\", \"hug\"] Corpus: (\"hug\", 10), (\"p\" \"##u\" \"##g\", 5), (\"p\" \"##u\" \"##n\", 12), (\"b\" \"##u\" \"##n\", 4), (\"hu\" \"##gs\", 5) and we continue like this until we reach the desired vocabulary size.\nImplement: Detail\nAdvantages: This method reduces the occurrence of unknown tokens, enhancing the model’s ability to handle diverse and previously unseen inputs.\nApplication: WordPiece Encoding is essential for models that require high accuracy in token representation and robust handling of diverse inputs.\n2.3 Unigram Encoding The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.\nUnigram Encoding is a probabilistic model that treats each token as an independent unit, selecting tokens based on their probability of occurrence in the training data. Unlike other methods that build tokens by iteratively merging characters or sub-words, Unigram Encoding starts with a large vocabulary and iteratively prunes it to find the optimal set of tokens.\nExample:\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) Initial Vocabulary: Unigram Encoding begins with a large vocabulary that includes all possible tokens derived from the training data. This vocabulary often consists of individual characters, sub-words, and entire words.\n[\"h\", \"u\", \"g\", \"hu\", \"ug\", \"p\", \"pu\", \"n\", \"un\", \"b\", \"bu\", \"s\", \"hug\", \"gs\", \"ugs\"] Probability Assignment: Each token in the initial vocabulary is assigned a probability based on its frequency of occurrence in the training data. These probabilities help in evaluating the importance of each token.\nFor Example, Here are the frequencies of all the possible subwords in the vocabulary:\n(\"h\", 15) (\"u\", 36) (\"g\", 20) (\"hu\", 15) (\"ug\", 20) (\"p\", 17) (\"pu\", 17) (\"n\", 16) (\"un\", 16) (\"b\", 4) (\"bu\", 4) (\"s\", 5) (\"hug\", 15) (\"gs\", 5) (\"ugs\", 5) So, the sum of all frequencies is 210, and the probability of the subword “ug” is thus 20/210.\nIterative Pruning: The algorithm iteratively removes tokens that contribute the least to the model’s likelihood, recalculating probabilities at each step. This process continues until an optimal vocabulary size is achieved, balancing model complexity and performance.\nFor example, “pug” has the probability:\n[\"p\", \"u\", \"g\"] : 0.000389 [\"p\", \"ug\"] : 0.0022676 [\"pu\", \"g\"] : 0.0022676 So, “pug” would be tokenized as [“p”, “ug”] or [“pu”, “g”], depending on which of those segmentations is encountered first\nEach word in the corpus has a score, and the loss is the negative log likelihood of those scores — that is, the sum for all the words in the corpus of all the -log(P(word)).\nLet’s go back to our example with the following corpus:\n(\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5) The tokenization of each word with their respective scores is:\n\"hug\": [\"hug\"] (score 0.071428) \"pug\": [\"pu\", \"g\"] (score 0.007710) \"pun\": [\"pu\", \"n\"] (score 0.006168) \"bun\": [\"bu\", \"n\"] (score 0.001451) \"hugs\": [\"hug\", \"s\"] (score 0.001701) So the loss is:\n10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8 Now we need to compute how removing each token affects the loss. removing “hug” will make the loss worse, because the tokenization of “hug” and “hugs” will become:\n\"hug\": [\"hu\", \"g\"] (score 0.006802) \"hugs\": [\"hu\", \"gs\"] (score 0.001701) These changes will cause the loss to rise by:\n- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5 Therefore, the token “pu” will probably be removed from the vocabulary, but not “hug”.\nFinal Vocabulary: The resulting vocabulary consists of tokens that maximize the likelihood of the training data. This vocabulary is used to tokenize new text inputs for the language model\nImplement: Detail\nUnigram Encoding is a powerful tokenization method that leverages probabilistic modeling to optimize the token vocabulary for language models. Its simplicity, efficiency, and flexibility make it a valuable tool in NLP, helping to create robust and versatile language models. By understanding and applying Unigram Encoding, we can enhance the performance and adaptability of our language processing systems.\n2.4 SentencePiece Encoding All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, SentencePiece treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.\nAdvantages: This approach is highly versatile, making it suitable for languages with complex word boundaries and noisy text data.\nApplication: SentencePiece is particularly useful in multilingual models and scenarios where the input text includes noise, such as OCR outputs or social media text.\n  Shervin Minaee et al. “Large Language Models: A Survey”. In: arXiv preprint arXiv:2402.06196 (2024). ↩︎\n   ","wordCount":"1956","inLanguage":"en","datePublished":"2024-06-10T00:00:00Z","dateModified":"2024-06-10T00:00:00Z","author":{"@type":"Person","name":"Thang Nguyen-Duc"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nducthang.github.io/posts/2024-06-10-components-of-llm/"},"publisher":{"@type":"Organization","name":"Thang's Blog","logo":{"@type":"ImageObject","url":"https://nducthang.github.io/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nducthang.github.io/ accesskey=h title="Thang's Blog (Alt + H)"><img src=https://nducthang.github.io/icon.png alt aria-label=logo height=50>Thang's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nducthang.github.io/ title=Home><span>Home</span></a></li><li><a href=https://nducthang.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nducthang.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nducthang.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nducthang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nducthang.github.io/posts/>Posts</a></div><h1 class=post-title>[LLM 02] Data cleaning and Tokenizations</h1><div class=post-meta><span title="2024-06-10 00:00:00 +0000 UTC">June 10, 2024</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;Thang Nguyen-Duc</div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-data-cleaning aria-label="1. Data Cleaning">1. Data Cleaning</a><ul><li><a href=#11-data-filtering aria-label="1.1 Data Filtering">1.1 Data Filtering</a></li><li><a href=#12-deduplication aria-label="1.2 Deduplication">1.2 Deduplication</a></li><li><a href=#13-summary aria-label="1.3 Summary">1.3 Summary</a></li></ul></li><li><a href=#2-tokenizations aria-label="2. Tokenizations">2. Tokenizations</a><ul><li><a href=#21-byte-pair-encoding-bpe aria-label="2.1 Byte Pair Encoding (BPE)">2.1 Byte Pair Encoding (BPE)</a></li><li><a href=#22-word-piece-encoding aria-label="2.2 Word Piece Encoding">2.2 Word Piece Encoding</a></li><li><a href=#23-unigram-encoding aria-label="2.3 Unigram Encoding">2.3 Unigram Encoding</a></li><li><a href=#24-sentencepiece-encoding aria-label="2.4 SentencePiece Encoding">2.4 SentencePiece Encoding</a></li></ul></li></ul></div></details></div><div class=post-content><p>This article summarizes word knowledge from <cite>Large Language Models: A Survey <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite> . This is a series of articles continuing <a href=../2024-05-01-llm-large-language-model-families/>LLM 01 - Large Language Model Families</a></p><h1 id=1-data-cleaning>1. Data Cleaning<a hidden class=anchor aria-hidden=true href=#1-data-cleaning>#</a></h1><p>Data quality is pivotal to the performance of language models. Effective data cleaning techniques, such as filtering and deduplication, can significantly enhance model performance.</p><h2 id=11-data-filtering>1.1 Data Filtering<a hidden class=anchor aria-hidden=true href=#11-data-filtering>#</a></h2><p>Data filtering aims to enhance the quality of training data and improve the effectiveness of the trained language models. Common data filtering techniques include:</p><ol><li><p><strong>Removing Noise</strong>: This involves eliminating irrelevant or noisy data that might impair the model&rsquo;s ability to generalize. For instance, removing false information from the training data can reduce the likelihood of the model generating incorrect responses. Two mainstream approaches for quality filtering are classifier-based and heuristic-based frameworks.</p></li><li><p><strong>Handling Outliers</strong>: Identifying and managing outliers or anomalies in the data to prevent them from disproportionately influencing the model. This ensures that the model learns from typical data patterns rather than skewed anomalies.</p></li><li><p><strong>Addressing Imbalances</strong>: Balancing the distribution of classes or categories in the dataset to avoid biases and ensure fair representation. This is particularly important for responsible model training and evaluation.</p></li><li><p><strong>Text Preprocessing</strong>: Cleaning and standardizing text data by removing stop words, punctuation, or other elements that may not contribute significantly to the model’s learning. This step ensures that the data is uniform and free of unnecessary noise.</p></li><li><p><strong>Dealing with Ambiguitie</strong>s: Resolving or excluding ambiguous or contradictory data that might confuse the model during training. By clarifying these ambiguities, the model can provide more definite and reliable answers.</p></li></ol><h2 id=12-deduplication>1.2 Deduplication<a hidden class=anchor aria-hidden=true href=#12-deduplication>#</a></h2><p>Deduplication refers to the process of removing duplicate instances or repeated occurrences of the same data in a dataset. Duplicate data points can introduce biases in the model training process and reduce data diversity, as the model may overfit on those particular instances.</p><h2 id=13-summary>1.3 Summary<a hidden class=anchor aria-hidden=true href=#13-summary>#</a></h2><p>In summary, data cleaning, which includes both filtering and deduplication, is essential for improving the performance of language models. By ensuring high-quality, diverse, and representative training data, we can train more effective and reliable models. As demonstrated by the Falcon40B example, rigorous data cleaning can lead to significant advancements in model performance, highlighting the importance of these techniques in the development of large language models.</p><h1 id=2-tokenizations>2. Tokenizations<a hidden class=anchor aria-hidden=true href=#2-tokenizations>#</a></h1><p>Tokenization refers to the process of converting a sequence of text into smaller parts known as tokens. While the simplest tokenization tool merely splits text based on white space, more advanced tokenization tools rely on a word dictionary. However, this approach faces the challenge of out-of-vocabulary (OOV) words, as the tokenizer can only recognize words within its dictionary. To mitigate this issue and improve dictionary coverage, popular tokenizers for large language models (LLMs) are based on sub-words. These sub-words can be combined to form a large number of words, including those not seen during training or those in different languages. Here, we will explore three widely used tokenization methods: Byte Pair Encoding, WordPiece Encoding, and SentencePiece Encoding.</p><h2 id=21-byte-pair-encoding-bpe>2.1 Byte Pair Encoding (BPE)<a hidden class=anchor aria-hidden=true href=#21-byte-pair-encoding-bpe>#</a></h2><p>Byte Pair Encoding is a algorithm that uses frequent patterns at byte level to compress data.
For example, Our corpus contain words:</p><pre tabindex=0><code>low low low lower
</code></pre><p>So, we have base vocabulary is:</p><pre tabindex=0><code> [&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;]
</code></pre><p>After getting this base vocabulary, we add new tokens until the desired vocabulary size is reached by learning merges, which are rules to merge two elements of the existing vocabulary together into a new one.
At any step during the tokenizer training, the BPE algorithm will search for the most frequent pair of existing tokens (by “pair,” here we mean two consecutive tokens in a word). That most frequent pair is the one that will be merged, and we rinse and repeat for the next step.
Iterations:</p><pre tabindex=0><code>Vocabulary: [&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;]
Corpus: [(&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, 3), (&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;, 1)]
</code></pre><p>Then we look at pairs. The pair (&rsquo;l&rsquo;, &lsquo;o&rsquo;) is present in the word low and lower with total 4 times.
So, we have:</p><pre tabindex=0><code>Vocabulary: [&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;, &#39;lo&#39;]
Corpus: [(&#39;lo&#39;, &#39;w&#39;, 3), (&#39;lo&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;, 1)]
</code></pre><p>Then:</p><pre tabindex=0><code>Vocabulary: [&#39;l&#39;, &#39;o&#39;, &#39;w&#39;, &#39;e&#39;, &#39;r&#39;, &#39;lo&#39;, &#39;low&#39;]
Corpus: [(&#39;low&#39;, 3), (&#39;low&#39;, &#39;e&#39;, &#39;r&#39;, 1)]
</code></pre><p>And we continue like this until we reach the desired vocabulary size.</p><p>Implement: <a href=https://huggingface.co/learn/nlp-course/en/chapter6/5>Detail</a></p><p><strong>Advantages</strong>: BPE is effective in representing morphological variations of frequent words, as common prefixes and suffixes are well captured if they frequently appear in the training data.</p><p><strong>Application</strong>: BPE is widely used in models where efficient vocabulary management and flexibility to handle new words are crucial.</p><h2 id=22-word-piece-encoding>2.2 Word Piece Encoding<a hidden class=anchor aria-hidden=true href=#22-word-piece-encoding>#</a></h2><p>WordPiece Encoding is primarily used in well-known models like BERT and Electra. It aims to address the issue of unknown tokens (UNK) by ensuring all characters in the training data are represented in the vocabulary.</p><p>WordPiece starts from a small vocabulary including the special tokens used by the model and the initial alphabet. Since it identifies subwords by adding a prefix (like ## for BERT), each word is initially split by adding that prefix to all the characters inside the word. So, for instance, &ldquo;word&rdquo; gets split like this:</p><pre tabindex=0><code>w ##o ##r ##d
</code></pre><p>Thus, the initial alphabet contains all the characters present at the beginning of a word and the characters present inside a word preceded by the WordPiece prefix.
Then, again like BPE, WordPiece learns merge rules. The main difference is the way the pair to be merged is selected. Instead of selecting the most frequent pair, WordPiece computes a score for each pair, using the following formula:
$$score=(freq\_of\_pair)/(freq\_of\_first\_element \times freq\_of\_second\_element)$$</p><p>By dividing the frequency of the pair by the product of the frequencies of each of its parts, the algorithm prioritizes the merging of pairs where the individual parts are less frequent in the vocabulary.</p><p>Example: We have vocabulary and frequent of each words as follow:</p><pre tabindex=0><code>(&#34;hug&#34;, 10), (&#34;pug&#34;, 5), (&#34;pun&#34;, 12), (&#34;bun&#34;, 4), (&#34;hugs&#34;, 5)
</code></pre><p>The most frequent pair is ("##u", &ldquo;##g&rdquo;) (present 20 times), but the individual frequency of &ldquo;##u&rdquo; is very high, so its score is not the highest (it’s 1 / 36). All pairs with a &ldquo;##u&rdquo; actually have that same score (1 / 36), so the best score goes to the pair ("##g", &ldquo;##s&rdquo;) — the only one without a &ldquo;##u&rdquo; — at 1 / 20, and the first merge learned is ("##g", &ldquo;##s&rdquo;) -> ("##gs").</p><pre tabindex=0><code>Vocabulary: [&#34;b&#34;, &#34;h&#34;, &#34;p&#34;, &#34;##g&#34;, &#34;##n&#34;, &#34;##s&#34;, &#34;##u&#34;, &#34;##gs&#34;]
Corpus: (&#34;h&#34; &#34;##u&#34; &#34;##g&#34;, 10), (&#34;p&#34; &#34;##u&#34; &#34;##g&#34;, 5), (&#34;p&#34; &#34;##u&#34; &#34;##n&#34;, 12), (&#34;b&#34; &#34;##u&#34; &#34;##n&#34;, 4), (&#34;h&#34; &#34;##u&#34; &#34;##gs&#34;, 5)
</code></pre><p>Iterative: (&ldquo;h&rdquo;, &ldquo;##u&rdquo;) -> &ldquo;hu&rdquo;.</p><pre tabindex=0><code>Vocabulary: [&#34;b&#34;, &#34;h&#34;, &#34;p&#34;, &#34;##g&#34;, &#34;##n&#34;, &#34;##s&#34;, &#34;##u&#34;, &#34;##gs&#34;, &#34;hu&#34;]
Corpus: (&#34;hu&#34; &#34;##g&#34;, 10), (&#34;p&#34; &#34;##u&#34; &#34;##g&#34;, 5), (&#34;p&#34; &#34;##u&#34; &#34;##n&#34;, 12), (&#34;b&#34; &#34;##u&#34; &#34;##n&#34;, 4), (&#34;hu&#34; &#34;##gs&#34;, 5)
</code></pre><p>Then the next best score is shared by (&ldquo;hu&rdquo;, &ldquo;##g&rdquo;) and (&ldquo;hu&rdquo;, &ldquo;##gs&rdquo;) (with 1/15, compared to 1/21 for all the other pairs), so the first pair with the biggest score is merged:</p><pre tabindex=0><code>Vocabulary: [&#34;b&#34;, &#34;h&#34;, &#34;p&#34;, &#34;##g&#34;, &#34;##n&#34;, &#34;##s&#34;, &#34;##u&#34;, &#34;##gs&#34;, &#34;hu&#34;, &#34;hug&#34;]
Corpus: (&#34;hug&#34;, 10), (&#34;p&#34; &#34;##u&#34; &#34;##g&#34;, 5), (&#34;p&#34; &#34;##u&#34; &#34;##n&#34;, 12), (&#34;b&#34; &#34;##u&#34; &#34;##n&#34;, 4), (&#34;hu&#34; &#34;##gs&#34;, 5)
</code></pre><p>and we continue like this until we reach the desired vocabulary size.</p><p>Implement: <a href="https://huggingface.co/learn/nlp-course/en/chapter6/6?fw=pt">Detail</a></p><p><strong>Advantages</strong>: This method reduces the occurrence of unknown tokens, enhancing the model’s ability to handle diverse and previously unseen inputs.</p><p><strong>Application</strong>: WordPiece Encoding is essential for models that require high accuracy in token representation and robust handling of diverse inputs.</p><h2 id=23-unigram-encoding>2.3 Unigram Encoding<a hidden class=anchor aria-hidden=true href=#23-unigram-encoding>#</a></h2><p>The Unigram algorithm is often used in SentencePiece, which is the tokenization algorithm used by models like AlBERT, T5, mBART, Big Bird, and XLNet.</p><p>Unigram Encoding is a probabilistic model that treats each token as an independent unit, selecting tokens based on their probability of occurrence in the training data. Unlike other methods that build tokens by iteratively merging characters or sub-words, Unigram Encoding starts with a large vocabulary and iteratively prunes it to find the optimal set of tokens.</p><p>Example:</p><pre tabindex=0><code>(&#34;hug&#34;, 10), (&#34;pug&#34;, 5), (&#34;pun&#34;, 12), (&#34;bun&#34;, 4), (&#34;hugs&#34;, 5)
</code></pre><p><strong>Initial Vocabulary</strong>: Unigram Encoding begins with a large vocabulary that includes all possible tokens derived from the training data. This vocabulary often consists of individual characters, sub-words, and entire words.</p><pre tabindex=0><code>[&#34;h&#34;, &#34;u&#34;, &#34;g&#34;, &#34;hu&#34;, &#34;ug&#34;, &#34;p&#34;, &#34;pu&#34;, &#34;n&#34;, &#34;un&#34;, &#34;b&#34;, &#34;bu&#34;, &#34;s&#34;, &#34;hug&#34;, &#34;gs&#34;, &#34;ugs&#34;]
</code></pre><p><strong>Probability Assignment</strong>: Each token in the initial vocabulary is assigned a probability based on its frequency of occurrence in the training data. These probabilities help in evaluating the importance of each token.</p><p>For Example, Here are the frequencies of all the possible subwords in the vocabulary:</p><pre tabindex=0><code>(&#34;h&#34;, 15) (&#34;u&#34;, 36) (&#34;g&#34;, 20) (&#34;hu&#34;, 15) (&#34;ug&#34;, 20) (&#34;p&#34;, 17) (&#34;pu&#34;, 17) (&#34;n&#34;, 16)
(&#34;un&#34;, 16) (&#34;b&#34;, 4) (&#34;bu&#34;, 4) (&#34;s&#34;, 5) (&#34;hug&#34;, 15) (&#34;gs&#34;, 5) (&#34;ugs&#34;, 5)
</code></pre><p>So, the sum of all frequencies is 210, and the probability of the subword &ldquo;ug&rdquo; is thus 20/210.</p><p><strong>Iterative Pruning</strong>: The algorithm iteratively removes tokens that contribute the least to the model&rsquo;s likelihood, recalculating probabilities at each step. This process continues until an optimal vocabulary size is achieved, balancing model complexity and performance.</p><p>For example, &ldquo;pug&rdquo; has the probability:</p><pre tabindex=0><code>[&#34;p&#34;, &#34;u&#34;, &#34;g&#34;] : 0.000389
[&#34;p&#34;, &#34;ug&#34;] : 0.0022676
[&#34;pu&#34;, &#34;g&#34;] : 0.0022676
</code></pre><p>So, &ldquo;pug&rdquo; would be tokenized as [&ldquo;p&rdquo;, &ldquo;ug&rdquo;] or [&ldquo;pu&rdquo;, &ldquo;g&rdquo;], depending on which of those segmentations is encountered first</p><p>Each word in the corpus has a score, and the loss is the negative log likelihood of those scores — that is, the sum for all the words in the corpus of all the -log(P(word)).</p><p>Let’s go back to our example with the following corpus:</p><pre tabindex=0><code>(&#34;hug&#34;, 10), (&#34;pug&#34;, 5), (&#34;pun&#34;, 12), (&#34;bun&#34;, 4), (&#34;hugs&#34;, 5)
</code></pre><p>The tokenization of each word with their respective scores is:</p><pre tabindex=0><code>&#34;hug&#34;: [&#34;hug&#34;] (score 0.071428)
&#34;pug&#34;: [&#34;pu&#34;, &#34;g&#34;] (score 0.007710)
&#34;pun&#34;: [&#34;pu&#34;, &#34;n&#34;] (score 0.006168)
&#34;bun&#34;: [&#34;bu&#34;, &#34;n&#34;] (score 0.001451)
&#34;hugs&#34;: [&#34;hug&#34;, &#34;s&#34;] (score 0.001701)
</code></pre><p>So the loss is:</p><pre tabindex=0><code>10 * (-log(0.071428)) + 5 * (-log(0.007710)) + 12 * (-log(0.006168)) + 4 * (-log(0.001451)) + 5 * (-log(0.001701)) = 169.8
</code></pre><p>Now we need to compute how removing each token affects the loss. removing &ldquo;hug&rdquo; will make the loss worse, because the tokenization of &ldquo;hug&rdquo; and &ldquo;hugs&rdquo; will become:</p><pre tabindex=0><code>&#34;hug&#34;: [&#34;hu&#34;, &#34;g&#34;] (score 0.006802)
&#34;hugs&#34;: [&#34;hu&#34;, &#34;gs&#34;] (score 0.001701)
</code></pre><p>These changes will cause the loss to rise by:</p><pre tabindex=0><code>- 10 * (-log(0.071428)) + 10 * (-log(0.006802)) = 23.5
</code></pre><p>Therefore, the token &ldquo;pu&rdquo; will probably be removed from the vocabulary, but not &ldquo;hug&rdquo;.</p><p><strong>Final Vocabulary</strong>: The resulting vocabulary consists of tokens that maximize the likelihood of the training data. This vocabulary is used to tokenize new text inputs for the language model</p><p>Implement: <a href="https://huggingface.co/learn/nlp-course/en/chapter6/7?fw=pt">Detail</a></p><p>Unigram Encoding is a powerful tokenization method that leverages probabilistic modeling to optimize the token vocabulary for language models. Its simplicity, efficiency, and flexibility make it a valuable tool in NLP, helping to create robust and versatile language models. By understanding and applying Unigram Encoding, we can enhance the performance and adaptability of our language processing systems.</p><h2 id=24-sentencepiece-encoding>2.4 SentencePiece Encoding<a hidden class=anchor aria-hidden=true href=#24-sentencepiece-encoding>#</a></h2><p>All tokenization algorithms described so far have the same problem: It is assumed that the input text uses spaces to separate words. However, not all languages use spaces to separate words. One possible solution is to use language specific pre-tokenizers, e.g. XLM uses a specific Chinese, Japanese, and Thai pre-tokenizer). To solve this problem more generally, SentencePiece treats the input as a raw input stream, thus including the space in the set of characters to use. It then uses the BPE or unigram algorithm to construct the appropriate vocabulary.</p><p><strong>Advantages</strong>: This approach is highly versatile, making it suitable for languages with complex word boundaries and noisy text data.</p><p><strong>Application</strong>: SentencePiece is particularly useful in multilingual models and scenarios where the input text includes noise, such as OCR outputs or social media text.</p><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Shervin Minaee et al. “Large Language Models: A Survey”. In: arXiv preprint arXiv:2402.06196 (2024).&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://nducthang.github.io/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=next href=https://nducthang.github.io/posts/2024-06-01-text-generation-in-sd/><span class=title>Next »</span><br><span>My research aims to improve image and text quality in Stable Diffusion</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share [LLM 02] Data cleaning and Tokenizations on x" href="https://x.com/intent/tweet/?text=%5bLLM%2002%5d%20Data%20cleaning%20and%20Tokenizations&url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f&hashtags=LLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [LLM 02] Data cleaning and Tokenizations on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f&title=%5bLLM%2002%5d%20Data%20cleaning%20and%20Tokenizations&summary=%5bLLM%2002%5d%20Data%20cleaning%20and%20Tokenizations&source=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [LLM 02] Data cleaning and Tokenizations on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f&title=%5bLLM%2002%5d%20Data%20cleaning%20and%20Tokenizations"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [LLM 02] Data cleaning and Tokenizations on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [LLM 02] Data cleaning and Tokenizations on whatsapp" href="https://api.whatsapp.com/send?text=%5bLLM%2002%5d%20Data%20cleaning%20and%20Tokenizations%20-%20https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [LLM 02] Data cleaning and Tokenizations on telegram" href="https://telegram.me/share/url?text=%5bLLM%2002%5d%20Data%20cleaning%20and%20Tokenizations&url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share [LLM 02] Data cleaning and Tokenizations on ycombinator" href="https://news.ycombinator.com/submitlink?t=%5bLLM%2002%5d%20Data%20cleaning%20and%20Tokenizations&u=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-06-10-components-of-llm%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://nducthang.github.io/>Thang's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>