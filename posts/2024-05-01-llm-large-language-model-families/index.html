<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Large Language Model Families | Thang's Blog</title><meta name=keywords content="LLM"><meta name=description content="This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.
1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs."><meta name=author content="Thang Nguyen-Duc"><link rel=canonical href=https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/><link crossorigin=anonymous href=/assets/css/stylesheet.19ce2e35b67c1eb3628e21b8844ca2f72d2adf9d9c715e7e568a40f209923876.css integrity="sha256-Gc4uNbZ8HrNijiG4hEyi9y0q352ccV5+VopA8gmSOHY=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://nducthang.github.io/icon.png><link rel=icon type=image/png sizes=16x16 href=https://nducthang.github.io/icon.png><link rel=icon type=image/png sizes=32x32 href=https://nducthang.github.io/icon.png><link rel=apple-touch-icon href=https://nducthang.github.io/icon.png><link rel=mask-icon href=https://nducthang.github.io/icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-MZBM3J5EX7"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-MZBM3J5EX7",{anonymize_ip:!1})}</script><meta property="og:title" content="Large Language Model Families"><meta property="og:description" content="This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.
1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs."><meta property="og:type" content="article"><meta property="og:url" content="https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-05-01T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Large Language Model Families"><meta name=twitter:description content="This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.
1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://nducthang.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Large Language Model Families","item":"https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Large Language Model Families","name":"Large Language Model Families","description":"This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.\n1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs.","keywords":["LLM"],"articleBody":"This article summarizes word knowledge from Large Language Models: A Survey 1 . In this section, I summarize about Large Language Model Families.\n1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs. Based on Transformer, we can split neural networks in LLM into 3 categories:\n Encoder-Only Decoder-Only Encoder-Decoder Only  1.1 Encoder-Only The models only consist of an encoder network. Representative encode-only models include BERT and its variants (RoBERTa, DeBERTa, XLM, ALBERT, …)\n BERT (Bidirectional Encoder Representations from Transformers) is one of the most widely used encode-only. The pre-trained BERT can be fine-tuned for different tasks like classification, question answering…  BERT consists of 3 modules:\n Embedding module: Convert input text into a sequence of embedding vectors. Stack of Transformer encoders: Converts embedding vectors into contextual representation vectors. Fully connected layer: Converts the representation vector (at the final layer) to one-hot vectors.  BERT training with 2 objectives.\n Masked Language Modeling (MLM) Next Sentence prediction (NSP)   RoBERTa: improves the robustness of BERT by modifying a few key hyperparameters, removing NSP, and enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. ALBERT uses 2 parameter-reduction technologies to lower memory consumption and increase the training speed of BERT:  Splitting the embedding matrix into 2 smaller matrices. Using repeating layers split among groups.   DeBERTa (Decoding enhanced BERT with disentangled attention): improve BERT and RoBERTa use 2 novel techniques:  The disentangled attention mechanism: Each word is represented using 2 vectors that encode its content and position. An enhanced mask recorder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. l adversarial training method is used for fine-tuning to improve models’ generalization.   ELECTRAL: New pre-trained task - Replaced token detection (RTD): Instead of masking the input, RDT corrupts it by replacing some tokens with plausible alternatives sampled from a small generator. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not. XLM: Extended BERT to cross-lingual language models using 2 methods:  An Unsupervised method that only relies on monolingual data. A supervised method that leverages parallel data with a new cross-lingual language model objective.    1.2 Decoder-Only The most widely used decoder-only are GPT-1 and GPT-2, these models lay the foundation for more powerful LLMs subsequently (i.e. GPT-3, GPT-4).\n GPT-1: (Generative Pre-Training) is a decoder-only Transformer model on diverse corpus of unlabeled text in self-supervised learning fashion, followed by discriminative fine-tuning on each specific downstream tasks.   GPT-2: shows that language models are able to learn to perform specific natural language tasks without any explicit supervision when trained on a large WebText dataset consisting of millions of webpages. The GPT-2 model follows the model designs of GPT-1 with a few modifications:  Layer normalization is moved to the input of each sub-block, additional layer normalization is added after the final self-attention block. Context size is increased from 512 to 1024 tokens.    1.3 Encoder-Decoder Only The most widely used encoder-decoder only are T5, mT5, MASSS and BART.\n T5: Text-to-Text Transfer Transformer (T5) model where transfer learning is effectively exploited for NLP via an introduction of a unified framework in which all NLP tasks are cast as a text-to-text generation task. mT5: pre-trained on a newcommon Crawl based dataset consisting of texts in 101 languages. MASS: MAsked Sequence to Sequence pre-training: adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence. The encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and the decoder predicts the masked fragment. BART: sequence-to-sequence translation model architecture, is a pre-trained by corrupting text with an arbitrary nosing function, and then learning to reconstruct the original text.  2. Large Languagme Model Families In this section, I review 3 LLM families: GPT, LLaMA, PaLM. In addition to these models, there are other popular LLMs which do not belong to those three model families, yet they have achieved great performance and have pushed the LLMs field forward: FLAN, Gopher, T0, ERNIE 3.0, RETRO, GlaM, LaMDA, OPTChinchilla, Galactica, CodeGen, AlexaTM, Sparrow, Minerva, MoD, BLOOM, GLM, Pythia, Orca, StarCoder, Kosmos, Gemini,…\n2.1 GPT Family This family consists of GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, GPT-4, CODEX, and WebGPT, developed by OpenAI.\n GPT-3: is a pre-trained autoregressive language model with 175 millio parameters. GPT-3 shows the emergent ability of in-context learning, which means GPT-3 can be applied to any downstream tasks without any gradient updates or fine-tuning. CodeX: is a general-purpose programming model that can parse natural language and generate code in response. Fine-tuned for programming applications on code corpora collected from GitHub. WebGPT: fine-tuned to answer open-ended questions using a text-based web browser, facilitating users to search and navigate the web. WebGPT is trained in 3 steps:  Firstly, WebGPT learn to mimic human browsing behaviors using human demonstration data. Secondly, a reward funtion is leared to predict human preferences. Finally, WebGPT is refined to optimize the reward function via reinforcemence learning and rejction sampling.   InstructGPT: is proposed to align language models with user intent on a wide range of tasks by fune-tuning with human feedback. The method is called Reinforcement Learning from Human Feedback (RLHF).  GPT-4: is a multimodal LLM in that it can take image and text as inputs and produce text outputs. GPT-4 was first pre-trained to predict next tokens on large text corpora, and then fine-tuned with RLHF to align model behaviors with human-desired ones.  2.2 LLaMA Family LLaMA is a collection of foundation language models, released by Meta. Unlike GPT models, LLaMA models are open-source.\n LLaMA: uses the transformer architecture of GPT-3, with a few minor architectural modifications, including:  Using a SwiGLU activation function instead of ReLU. Using rotary postional embeddings instead of absolute positional embedding. Using root-mean-squared layer normalization instead of standard layer nomialization.   LLaMA 2: include both foundation language models and Chat models funetuned for dialog (LLaMA-2 Chat)  Alpaca: is fune-tuned from the LLaMA-7B model using 52K instruction-following demonstations generated in style of self-instruct using GPT-3.5. Vicuna: fine-tuning LLaMA on user-shared conversations. Guanaco: also finetuned LLaMA models using instruction-following data. But the funetuning a 65B done a a single 48GB GPU with QLoRA. QLoRA back-propagates gradients through a frozen, 4-bit quantized pre-trained language model into Low Rank Adapters (LoRA). Mistral-7B: is a 7B parameter language model. . This model leverages grouped-query attention for faster inference, coupled with sliding window attention to effectively handle sequences of arbitrary length with a reduced inference cost.  2.3 PaLM Family The PaLM (Pathways Language Model) family are developed by Google. It is a 540B parameter transformer-based LLM, trained on high-quality text corpus consistaning of 780 billion tokens.\n U-PaLM: models of 8B, 62B, and 540B scales arecontinually trained on PaLM with UL2R, a method of continue training LLMs on a few steps with UL2’s mixture-of-denoiser objective. PaLM-2: is trained using a mixture of objectives. Through extensive evaluations on English, multilingual, and reasoning tasks. Med-PaLM: is a medical questions domainm finetuned on PaLM using instruction prompt tuning, a parameter-effectient method for aligning LLMs to new domains using a few examples.    Shervin Minaee et al. “Large Language Models: A Survey”. In: arXiv preprint arXiv:2402.06196 (2024). ↩︎\n   ","wordCount":"1252","inLanguage":"en","datePublished":"2024-05-01T00:00:00Z","dateModified":"2024-05-01T00:00:00Z","author":{"@type":"Person","name":"Thang Nguyen-Duc"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/"},"publisher":{"@type":"Organization","name":"Thang's Blog","logo":{"@type":"ImageObject","url":"https://nducthang.github.io/icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://nducthang.github.io/ accesskey=h title="Thang's Blog (Alt + H)"><img src=https://nducthang.github.io/icon.png alt aria-label=logo height=50>Thang's Blog</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://nducthang.github.io/ title=Home><span>Home</span></a></li><li><a href=https://nducthang.github.io/archives title=Archive><span>Archive</span></a></li><li><a href=https://nducthang.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://nducthang.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://nducthang.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://nducthang.github.io/posts/>Posts</a></div><h1 class=post-title>Large Language Model Families</h1><div class=post-meta><span title="2024-05-01 00:00:00 +0000 UTC">May 1, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Thang Nguyen-Duc</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#1-basic-architecture aria-label="1. Basic Architecture">1. Basic Architecture</a><ul><li><a href=#11-encoder-only aria-label="1.1 Encoder-Only">1.1 Encoder-Only</a></li><li><a href=#12-decoder-only aria-label="1.2 Decoder-Only">1.2 Decoder-Only</a></li><li><a href=#13-encoder-decoder-only aria-label="1.3 Encoder-Decoder Only">1.3 Encoder-Decoder Only</a></li></ul></li><li><a href=#2-large-languagme-model-families aria-label="2. Large Languagme Model Families">2. Large Languagme Model Families</a><ul><li><a href=#21-gpt-family aria-label="2.1 GPT Family">2.1 GPT Family</a></li><li><a href=#22-llama-family aria-label="2.2 LLaMA Family">2.2 LLaMA Family</a></li><li><a href=#23-palm-family aria-label="2.3 PaLM Family">2.3 PaLM Family</a></li></ul></li></ul></div></details></div><div class=post-content><p>This article summarizes word knowledge from <cite>Large Language Models: A Survey <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup></cite> . In this section, I summarize about Large Language Model Families.</p><h1 id=1-basic-architecture>1. Basic Architecture<a hidden class=anchor aria-hidden=true href=#1-basic-architecture>#</a></h1><p>The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another. Transformers allow for much more parallelization than RNNs.
<img loading=lazy src=1.png alt="Transformer architecture">
Based on Transformer, we can split neural networks in LLM into 3 categories:</p><ul><li>Encoder-Only</li><li>Decoder-Only</li><li>Encoder-Decoder Only</li></ul><h2 id=11-encoder-only>1.1 Encoder-Only<a hidden class=anchor aria-hidden=true href=#11-encoder-only>#</a></h2><p>The models only consist of an encoder network. Representative encode-only models include BERT and its variants (RoBERTa, DeBERTa, XLM, ALBERT, …)</p><ul><li><em><strong>BERT</strong> (Bidirectional Encoder Representations from Transformers)</em> is one of the most widely used encode-only. The pre-trained BERT can be fine-tuned for different tasks like classification, question answering…</li></ul><p><img loading=lazy src=2.png alt="BERT model">
<img loading=lazy src=3.png alt="BERT fintune"></p><p>BERT consists of 3 modules:</p><ol><li>Embedding module: Convert input text into a sequence of embedding vectors.</li><li>Stack of Transformer encoders: Converts embedding vectors into contextual representation vectors.</li><li>Fully connected layer: Converts the representation vector (at the final layer) to one-hot vectors.</li></ol><p>BERT training with 2 objectives.</p><ol><li>Masked Language Modeling (MLM)</li><li>Next Sentence prediction (NSP)</li></ol><ul><li><em><strong>RoBERTa</strong></em>: improves the robustness of BERT by modifying a few key hyperparameters, removing NSP, and enhanced mask decoder is used to incorporate absolute positions in the decoding
layer to predict the masked tokens in model pre-training.</li><li><em><strong>ALBERT</strong></em> uses 2 parameter-reduction technologies to lower memory consumption and increase the training speed of BERT:<ol><li>Splitting the embedding matrix into 2 smaller matrices.</li><li>Using repeating layers split among groups.</li></ol></li><li><em><strong>DeBERTa</strong></em> (Decoding enhanced BERT with disentangled attention): improve BERT and RoBERTa use 2 novel techniques:<ol><li>The disentangled attention mechanism: Each word is represented using 2 vectors that encode its content and position.</li><li>An enhanced mask recorder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. l adversarial training method is used for fine-tuning to improve models’ generalization.</li></ol></li><li><em><strong>ELECTRAL</strong></em>: New pre-trained task - Replaced token detection (RTD): Instead of masking the input, RDT corrupts it by replacing some tokens with plausible alternatives sampled from a small generator. Then, instead of training a model that predicts the original identities of the corrupted tokens, a discriminative model is trained to predict whether a token in the corrupted input was replaced by a generated sample or not.</li><li><em><strong>XLM</strong></em>: Extended BERT to cross-lingual language models using 2 methods:<ol><li>An Unsupervised method that only relies on monolingual data.</li><li>A supervised method that leverages parallel data with a new cross-lingual language model objective.</li></ol></li></ul><p><img loading=lazy src=4.png alt="Cross-lingual language model pretraining. The MLM objective is similar to BERT but with continuous streams of text as opposed to sentence pairs. The TLM objective extends MLM to pairs of parallel sentences. To predict a masked English word, the model can attend to both the English sentence and its French translation and is encouraged to align English and French representations"></p><h2 id=12-decoder-only>1.2 Decoder-Only<a hidden class=anchor aria-hidden=true href=#12-decoder-only>#</a></h2><p>The most widely used decoder-only are GPT-1 and GPT-2, these models lay the foundation for more powerful LLMs subsequently (i.e. GPT-3, GPT-4).</p><ul><li><em><strong>GPT-1</strong></em>: (Generative Pre-Training) is a decoder-only Transformer model on diverse corpus of unlabeled text in self-supervised learning fashion, followed by discriminative fine-tuning on each specific downstream tasks.</li></ul><p><img loading=lazy src=5.png alt="GPT 1"></p><ul><li><em><strong>GPT-2</strong></em>: shows that language models are able to learn to perform specific natural language tasks without any explicit supervision when trained on a large WebText dataset consisting of millions of webpages. The GPT-2 model follows the model designs of GPT-1 with a few modifications:<ul><li>Layer normalization is moved to the input of each sub-block, additional layer normalization is added after the final self-attention block.</li><li>Context size is increased from 512 to 1024 tokens.</li></ul></li></ul><h2 id=13-encoder-decoder-only>1.3 Encoder-Decoder Only<a hidden class=anchor aria-hidden=true href=#13-encoder-decoder-only>#</a></h2><p>The most widely used encoder-decoder only are T5, mT5, MASSS and BART.</p><ul><li><em><strong>T5</strong></em>: Text-to-Text Transfer Transformer (T5) model where transfer learning is effectively exploited for NLP via an introduction of a unified framework in which all NLP tasks are cast as a text-to-text generation task.</li><li><em><strong>mT5</strong></em>: pre-trained on a newcommon Crawl based dataset consisting of texts in 101 languages.</li><li><em><strong>MASS</strong></em>: MAsked Sequence to Sequence pre-training: adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence. The encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and the decoder predicts the masked fragment.</li><li><em><strong>BART</strong></em>: sequence-to-sequence translation model architecture, is a pre-trained by corrupting text with an arbitrary nosing function, and then learning to reconstruct the original text.</li></ul><h1 id=2-large-languagme-model-families>2. Large Languagme Model Families<a hidden class=anchor aria-hidden=true href=#2-large-languagme-model-families>#</a></h1><p>In this section, I review 3 LLM families: GPT, LLaMA, PaLM.
<img loading=lazy src=6.png alt="Large Languagme Model Families"></p><p>In addition to these models, there are other popular LLMs which do not belong to those three model families, yet they have achieved great performance and have pushed the LLMs field forward: FLAN, Gopher, T0, ERNIE 3.0, RETRO, GlaM, LaMDA, OPTChinchilla, Galactica, CodeGen, AlexaTM, Sparrow, Minerva, MoD, BLOOM, GLM, Pythia, Orca, StarCoder, Kosmos, Gemini,…</p><h2 id=21-gpt-family>2.1 GPT Family<a hidden class=anchor aria-hidden=true href=#21-gpt-family>#</a></h2><p>This family consists of GPT-1, GPT-2, GPT-3, InstructGPT, ChatGPT, GPT-4, CODEX, and WebGPT, developed by OpenAI.</p><ul><li><em><strong>GPT-3</strong></em>: is a pre-trained autoregressive language model with 175 millio parameters. GPT-3 shows the emergent ability of in-context learning, which means GPT-3 can be applied to any downstream tasks without any gradient updates or fine-tuning.</li><li><em><strong>CodeX</strong></em>: is a general-purpose programming model that can parse natural language and generate code in response. Fine-tuned for programming applications on code corpora collected from GitHub.</li><li><em><strong>WebGPT</strong></em>: fine-tuned to answer open-ended questions using a text-based web browser, facilitating users to search and navigate the web. WebGPT is trained in 3 steps:<ul><li>Firstly, WebGPT learn to mimic human browsing behaviors using human demonstration data.</li><li>Secondly, a reward funtion is leared to predict human preferences.</li><li>Finally, WebGPT is refined to optimize the reward function via reinforcemence learning and rejction sampling.</li></ul></li><li><em><strong>InstructGPT</strong></em>: is proposed to align language models with user intent on a wide range of tasks by fune-tuning with human feedback. The method is called Reinforcement Learning from Human Feedback (RLHF).
<img loading=lazy src=7.png alt="Reinforcement Learning from Human Feedback (RLHF)"></li><li><em><strong>GPT-4</strong></em>: is a multimodal LLM in that it can take image and text as inputs and produce text outputs. GPT-4 was first pre-trained to predict next tokens on large text corpora, and then fine-tuned with RLHF to align model behaviors with human-desired ones.</li></ul><h2 id=22-llama-family>2.2 LLaMA Family<a hidden class=anchor aria-hidden=true href=#22-llama-family>#</a></h2><p>LLaMA is a collection of foundation language models, released by Meta. Unlike GPT models,
LLaMA models are open-source.</p><ul><li><em><strong>LLaMA</strong></em>: uses the transformer architecture of GPT-3, with a few minor architectural modifications, including:<ol><li>Using a SwiGLU activation function instead of ReLU.</li><li>Using rotary postional embeddings instead of absolute positional embedding.</li><li>Using root-mean-squared layer normalization instead of standard layer nomialization.</li></ol></li><li><em><strong>LLaMA 2</strong></em>: include both foundation language models and Chat models funetuned for dialog (LLaMA-2 Chat)
<img loading=lazy src=8.png alt="Training of LLaMA-2 Chat"></li><li><em><strong>Alpaca</strong></em>: is fune-tuned from the LLaMA-7B model using 52K instruction-following demonstations generated in style of self-instruct using GPT-3.5.</li><li><em><strong>Vicuna</strong></em>: fine-tuning LLaMA on user-shared conversations.</li><li><em><strong>Guanaco</strong></em>: also finetuned LLaMA models using instruction-following data. But the funetuning a 65B done a a single 48GB GPU with QLoRA. QLoRA back-propagates gradients through a frozen, 4-bit quantized pre-trained language model into Low Rank Adapters (LoRA).</li><li><em><strong>Mistral-7B</strong></em>: is a 7B parameter language model. . This model leverages grouped-query attention for faster inference, coupled with sliding window attention to effectively handle sequences of arbitrary length with a reduced inference cost.</li></ul><h2 id=23-palm-family>2.3 PaLM Family<a hidden class=anchor aria-hidden=true href=#23-palm-family>#</a></h2><p>The PaLM (Pathways Language Model) family are developed by Google. It is a 540B parameter transformer-based LLM, trained on high-quality text corpus consistaning of 780 billion tokens.</p><ul><li><em><strong>U-PaLM</strong>:</em> models of 8B, 62B, and 540B scales arecontinually trained on PaLM with UL2R, a method of continue training LLMs on a few steps with UL2’s mixture-of-denoiser objective.</li><li><em><strong>PaLM-2</strong></em>: is trained using a mixture of objectives. Through extensive evaluations on English, multilingual, and reasoning tasks.</li><li><em><strong>Med-PaLM</strong></em>: is a medical questions domainm finetuned on PaLM using instruction prompt tuning, a parameter-effectient method for aligning LLMs to new domains using a few examples.</li></ul><section class=footnotes role=doc-endnotes><hr><ol><li id=fn:1 role=doc-endnote><p>Shervin Minaee et al. “Large Language Models: A Survey”. In: arXiv preprint arXiv:2402.06196 (2024).&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></section></div><footer class=post-footer><ul class=post-tags><li><a href=https://nducthang.github.io/tags/llm/>LLM</a></li></ul><nav class=paginav><a class=next href=https://nducthang.github.io/posts/2023-11-25-gradient-based-methods/><span class=title>Next »</span><br><span>Gradient-based methods in error detection</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Families on x" href="https://x.com/intent/tweet/?text=Large%20Language%20Model%20Families&url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f&hashtags=LLM"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Families on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f&title=Large%20Language%20Model%20Families&summary=Large%20Language%20Model%20Families&source=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Families on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f&title=Large%20Language%20Model%20Families"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Families on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Families on whatsapp" href="https://api.whatsapp.com/send?text=Large%20Language%20Model%20Families%20-%20https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Families on telegram" href="https://telegram.me/share/url?text=Large%20Language%20Model%20Families&url=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Large Language Model Families on ycombinator" href="https://news.ycombinator.com/submitlink?t=Large%20Language%20Model%20Families&u=https%3a%2f%2fnducthang.github.io%2fposts%2f2024-05-01-llm-large-language-model-families%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://nducthang.github.io/>Thang's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(t=>{const n=t.parentNode.parentNode,e=document.createElement("button");e.classList.add("copy-code"),e.innerHTML="copy";function s(){e.innerHTML="copied!",setTimeout(()=>{e.innerHTML="copy"},2e3)}e.addEventListener("click",o=>{if("clipboard"in navigator){navigator.clipboard.writeText(t.textContent),s();return}const e=document.createRange();e.selectNodeContents(t);const n=window.getSelection();n.removeAllRanges(),n.addRange(e);try{document.execCommand("copy"),s()}catch(e){}n.removeRange(e)}),n.classList.contains("highlight")?n.appendChild(e):n.parentNode.firstChild==n||(t.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?t.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(e):t.parentNode.appendChild(e))})</script></body></html>