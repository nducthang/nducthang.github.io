<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Thang&#39;s Blog</title>
    <link>https://nducthang.github.io/posts/</link>
    <description>Recent content in Posts on Thang&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 01 May 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://nducthang.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Large Language Model Families</title>
      <link>https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/</link>
      <pubDate>Wed, 01 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://nducthang.github.io/posts/2024-05-01-llm-large-language-model-families/</guid>
      <description>Source: [Large Language Models: A Survey ^1]
This article summarizes word knowledge from Large Language Models: A Survey. In this section, I summarize about Large Language Model Families.
1. Basic Architecture The invention of the Transformer architecture marks another milestone in the development of LLM. By applying self-attention to compute in parallel for every word in a sentence of the document an “attention score” to model the influence each word has on another.</description>
    </item>
    
    <item>
      <title>Gradient-based methods in error detection</title>
      <link>https://nducthang.github.io/posts/2023-11-25-gradient-based-methods/</link>
      <pubDate>Sat, 25 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://nducthang.github.io/posts/2023-11-25-gradient-based-methods/</guid>
      <description>I strongly encourage you to explore my earlier article, Understanding the Influence Function, as it serves as a valuable foundation for comprehending the content presented in this piece.
 What is error detection problem? The rapid growth of the internet, however, causes data to rise exponentially, posing numerous issues. Deep learning algorithms become less effective when big data is mislabeled or contains many errors. Current studies focus solely on improving the model rather than detecting data issues.</description>
    </item>
    
    <item>
      <title>What is Influence Function?</title>
      <link>https://nducthang.github.io/posts/2023-11-24-influence-function/</link>
      <pubDate>Fri, 24 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://nducthang.github.io/posts/2023-11-24-influence-function/</guid>
      <description>In this article, I review about Influence functions and various of its - a classic technique from robust statistics - to trace a model&amp;rsquo;s prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction.
Basics of influence function Consider a prediction problem from some input space \(\mathcal{X}\) (e.g., images, text,\(\ldots\)) to an output space \(\mathcal{Y}\) (e.g,. labels).</description>
    </item>
    
  </channel>
</rss>
